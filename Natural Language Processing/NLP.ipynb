{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Creado Por Jorge Cardona</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# configura el tamano de cada celda al 100% de la pantalla\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.15</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importa las librerias que se van a necesitar en el procesamiento de datos\n",
    "\n",
    "# importa pandas, que es una libreria para análisis de datos que proporciona unas estructuras de datos flexibles\n",
    "import pandas as pd\n",
    "\n",
    "# libreria que permite usar las operaciones matematicas sobre variables\n",
    "import math\n",
    "\n",
    "# libreria que sirve para articionar aleatoriamente los datasets\n",
    "import random\n",
    "\n",
    "# libreria que permite obtener los caracteres del codigo ASCII, con el metodo string.printable\n",
    "import string\n",
    "\n",
    "# libreria para realizar debug\n",
    "import pixiedust\n",
    "\n",
    "#libreria para expresiones regulares\n",
    "import re\n",
    "\n",
    "# libreria para revision ortografica\n",
    "from spellchecker import SpellChecker\n",
    "   \n",
    "# importa os que permite leer variables del sistema operativo y manipular archivos del sistema\n",
    "import os\n",
    "\n",
    "# importa la libreria que permite hacer solicitudes\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion que permite obtener el contenido delarchivo leido\n",
    "def obtener_datos_de_archivo (archivo_lectura):\n",
    "    \n",
    "    # muestra todo el texto de la columna\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "    # asigna el directorio de donde se va a leer el archivo\n",
    "    archivo = archivo_lectura\n",
    "\n",
    "    # lee el archivo y lo almacena en un dataFrame\n",
    "    df = pd.read_excel(archivo)\n",
    "\n",
    "    # reemplaza los saltos de linea con espacios en blanco\n",
    "    df = df.replace('\\n',' ', regex=True)\n",
    "\n",
    "    # reemplaza los NaN con cadena vacia\n",
    "    df = df.fillna('')\n",
    "    \n",
    "    # retorna el contenido cargado en la variable\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion que permite obtener los limites paa lectura del archivo leido\n",
    "def obtener_datos_para_control_de_contenido (df_original):\n",
    "    \n",
    "    # lee la linea donde estan las etiquetas de la tabla\n",
    "    etiquetados = df_original.iloc[:0,]\n",
    "\n",
    "    # obtiene los nombres de las columnas para procesar\n",
    "    labels = df_original.columns.values\n",
    "\n",
    "    # listado de columnas a eliminar\n",
    "    archivoDepurado= df_original.drop(columns=['ARCHIVO','COMPLEMENTACION']) # no se tienen en cuenta ARCHIVO\tOCR FULL\tCOMPLEMENTACION\n",
    "\n",
    "    # obtiene la cantidad de filas a recorrer\n",
    "    cantidad_filas    = archivoDepurado.shape[0] \n",
    "\n",
    "    # obtiene la cantidad de columnas a recorrer\n",
    "    cantidad_columnas = (archivoDepurado.shape[1]) \n",
    "\n",
    "    # obtiene la cantidad de columnas a evaluar\n",
    "    cantidad_propietarios = int(cantidad_columnas/5) # se divide en 5 ya que son 5 campos por propietario, debe dar numero exacto\n",
    "\n",
    "    # obtiene los nombres de los labels para separar las consultas\n",
    "    labels = pd.Series(archivoDepurado.columns.values).tolist()\n",
    "    \n",
    "    return cantidad_filas,cantidad_propietarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "def obtener_listado_nombres_apellidos_corporaciones(fila, columnas_totales, NOMBRE_ARCHIVO_LEIDO, TEXTO_COMPLEMENTACION):\n",
    "    \n",
    "    # define que la etiqueta contenga como minimo los carateres definidos\n",
    "    TAMANO_MINIMO_TEXTO_ETIQUETADO                 = 5  \n",
    "    \n",
    "    # define que los nombres y apellidos etiquetados contengan por lo menos el numero de caracteres designado\n",
    "    TAMANO_MINIMO_NOMBRES_APELLIDOS_CORPORACIONES  = 3  \n",
    "    \n",
    "    # almacena los listados de corporaciones, nombres, apellidos leidos en la fila del archivo\n",
    "    SIMBOLO_PREPROCESAMIENTO   = '▓'\n",
    "    LISTADO_CORPORACIONES      = []\n",
    "    LISTADO_NOMBRES            = []\n",
    "    LISTADO_APELLIDOS          = [] \n",
    "    COMPLEMENTACION            = TEXTO_COMPLEMENTACION\n",
    "    # preprocesamiento, cambio de caracter coma ',' por el SIMBOLO_PREPROCESAMIENTO para evitar fallos al usar el contenido de informacion en vectores\n",
    "    COMPLEMENTACION            = COMPLEMENTACION.replace(',',SIMBOLO_PREPROCESAMIENTO) \n",
    "    COMPLEMENTACION            = COMPLEMENTACION.replace(\"\\'\",'').replace('\\\\','').replace('\"', '') \n",
    "    COMPLEMENTACION_TOKENIZADA = COMPLEMENTACION\n",
    "    \n",
    "    \n",
    "    # recorre cada columna de la fila para leer el contenido y definir si es nombre, apellido o corporacion\n",
    "    for columna in range(1, columnas_totales):\n",
    "        \n",
    "        # complemento de la columna a leer del archivo\n",
    "        ETIQUETA                         = str(columna) + '_ETIQUETA_COMPLETA'     \n",
    "        \n",
    "        # obtiene la columna del dataframe\n",
    "        COLUMNA_LEIDA_DATAFRAME          = df_original[ETIQUETA]\n",
    "        # preprocesamiento, cambio de caracter coma ',' por el SIMBOLO_PREPROCESAMIENTO para evitar fallos al usar el contenido de informacion en vectores\n",
    "        COLUMNA_LEIDA_DATAFRAME          = COLUMNA_LEIDA_DATAFRAME.replace(',',SIMBOLO_PREPROCESAMIENTO) \n",
    "\n",
    "        # obtiene la informacion de texto etiquetado\n",
    "        TEXTO_ETIQUETADO                 = COLUMNA_LEIDA_DATAFRAME.iloc[fila:fila + 1].to_string(index=False).upper()\n",
    "               \n",
    "        # Verifica que la columna leida X_ETIQUETA_COMPLETA tenga contenido y que al menos tenga 5 caracteres\n",
    "        if((TEXTO_ETIQUETADO != '') and (len(TEXTO_ETIQUETADO) > TAMANO_MINIMO_TEXTO_ETIQUETADO)):\n",
    "            \n",
    "            # obtiene la informacion de cada etiqueta del archivo y la almacena en la variable correspondiente, para realizar las validaciones siguientes\n",
    "            NOMBRE_1                         = df_original[str(columna) + '_ETIQUETA_NOMBRE_1']   .iloc[fila:fila + 1].to_string(index=False).upper().strip()\n",
    "            NOMBRE_2                         = df_original[str(columna) + '_ETIQUETA_NOMBRE_2']   .iloc[fila:fila + 1].to_string(index=False).upper().strip()\n",
    "            APELLIDO_1                       = df_original[str(columna) + '_ETIQUETA_APELLIDO_1'] .iloc[fila:fila + 1].to_string(index=False).upper().strip()\n",
    "            APELLIDO_2                       = df_original[str(columna) + '_ETIQUETA_APELLIDO_2'] .iloc[fila:fila + 1].to_string(index=False).upper().strip()\n",
    "            CORPORACION                      = df_original[str(columna) + '_ETIQUETA_COMPLETA']   .iloc[fila:fila + 1].to_string(index=False).upper().strip()  \n",
    "            \n",
    "            # preprocesamiento, cambio de caracter coma ',' por el SIMBOLO_PREPROCESAMIENTO para evitar fallos al usar el contenido de informacion en vectores\n",
    "            NOMBRE_1                         = NOMBRE_1.replace(',',SIMBOLO_PREPROCESAMIENTO).replace(\"\\'\",'').replace('\\\\','').replace('\"', '')   \n",
    "            NOMBRE_2                         = NOMBRE_2.replace(',',SIMBOLO_PREPROCESAMIENTO).replace(\"\\'\",'').replace('\\\\','').replace('\"', '')  \n",
    "            APELLIDO_1                       = APELLIDO_1.replace(',',SIMBOLO_PREPROCESAMIENTO).replace(\"\\'\",'').replace('\\\\','').replace('\"', '') \n",
    "            APELLIDO_2                       = APELLIDO_2.replace(',',SIMBOLO_PREPROCESAMIENTO).replace(\"\\'\",'').replace('\\\\','').replace('\"', '') \n",
    "            CORPORACION                      = CORPORACION.replace(',',SIMBOLO_PREPROCESAMIENTO).replace(\"\\'\",'').replace('\\\\','').replace('\"', '')                     \n",
    "            \n",
    "            # verifica si no existen NOMBRES NI APELLIDOS, asociados a la X_ETIQUETA_COMPLETA leida\n",
    "            if( NOMBRE_1 == '' and NOMBRE_2 == '' and APELLIDO_1 == '' and APELLIDO_2 == ''):                  \n",
    "\n",
    "                # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos\n",
    "                if(len(NOMBRE_1) >= TAMANO_MINIMO_NOMBRES_APELLIDOS_CORPORACIONES):\n",
    "                    \n",
    "                    # adiciona las corporaciones encontradas al listado de corporaciones\n",
    "                    LISTADO_CORPORACIONES.append(CORPORACION)\n",
    "                \n",
    "            # verifica si solo existe NOMBRE_1 asociado a la X_ETIQUETA_COMPLETA leida\n",
    "            elif(NOMBRE_1 != '' and NOMBRE_2 == '' and APELLIDO_1 == '' and APELLIDO_2 == ''):\n",
    "                \n",
    "                # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos\n",
    "                if(len(NOMBRE_1) >= TAMANO_MINIMO_NOMBRES_APELLIDOS_CORPORACIONES):\n",
    "\n",
    "                    # adiciona las corporaciones encontradas al listado de corporaciones\n",
    "                    LISTADO_CORPORACIONES.append(CORPORACION)\n",
    "\n",
    "            # verifica si solo existen NOMBRE_1 y NOMBRE_2 asocioados a la X_ETIQUETA_COMPLETA leida\n",
    "            elif( NOMBRE_1 != '' and NOMBRE_2 != '' and APELLIDO_1 == '' and APELLIDO_2 == ''):                \n",
    "                               \n",
    "                # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos\n",
    "                if(len(NOMBRE_1) >= TAMANO_MINIMO_NOMBRES_APELLIDOS_CORPORACIONES):\n",
    "                    # adiciona los nombres encontradas al listado de nombres\n",
    "                    LISTADO_NOMBRES.append(NOMBRE_1)\n",
    "                    \n",
    "                # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos  \n",
    "                if(len(NOMBRE_2) >= TAMANO_MINIMO_NOMBRES_APELLIDOS_CORPORACIONES):\n",
    "                    # adiciona los nombres encontradas al listado de nombres\n",
    "                    LISTADO_NOMBRES.append(NOMBRE_2)\n",
    "                \n",
    "            # verifica si solo existe NOMBRE_1 y APELLIDO_1 asociados a la X_ETIQUETA_COMPLETA leida\n",
    "            elif(NOMBRE_1 != '' and NOMBRE_2 == '' and APELLIDO_1 != '' and APELLIDO_2 == ''):\n",
    "\n",
    "                # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos\n",
    "                if(len(NOMBRE_1) >= TAMANO_MINIMO_NOMBRES_APELLIDOS_CORPORACIONES):\n",
    "                    # adiciona los nombres encontradas al listado de nombres\n",
    "                    LISTADO_NOMBRES.append(NOMBRE_1)\n",
    "                    \n",
    "                # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos   \n",
    "                if(len(APELLIDO_1) >= TAMANO_MINIMO_NOMBRES_APELLIDOS_CORPORACIONES):\n",
    "                    # adiciona los apellidos encontradas al listado de apellidos\n",
    "                    LISTADO_APELLIDOS.append(APELLIDO_1)\n",
    "        \n",
    "            # verifica si solo existen NOMBRE_1, NOMBRE_2 y APELLIDO_1 asociados a la X_ETIQUETA_COMPLETA leida\n",
    "            elif( NOMBRE_1 != '' and NOMBRE_2 != '' and APELLIDO_1 != '' and APELLIDO_2 == ''):                 \n",
    "               \n",
    "                # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos\n",
    "                if(len(NOMBRE_1) >= TAMANO_MINIMO_NOMBRES_APELLIDOS_CORPORACIONES):\n",
    "                    # adiciona los nombres encontradas al listado de nombres\n",
    "                    LISTADO_NOMBRES.append(NOMBRE_1)\n",
    "                    \n",
    "                # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos\n",
    "                if(len(NOMBRE_2) >= TAMANO_MINIMO_NOMBRES_APELLIDOS_CORPORACIONES):\n",
    "                    # adiciona los nombres encontradas al listado de nombres\n",
    "                    LISTADO_NOMBRES.append(NOMBRE_2)\n",
    "                    \n",
    "                # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos   \n",
    "                if(len(APELLIDO_1) >= TAMANO_MINIMO_NOMBRES_APELLIDOS_CORPORACIONES):\n",
    "                    # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos\n",
    "                    LISTADO_APELLIDOS.append(APELLIDO_1)\n",
    "\n",
    "            # verifica si solo existe NOMBRE_2 y el APELLIDO_1 asociados a la X_ETIQUETA_COMPLETA leida\n",
    "            elif( NOMBRE_1 == '' and NOMBRE_2 != '' and APELLIDO_1 != '' and APELLIDO_2 == ''):                 \n",
    "               \n",
    "                # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos\n",
    "                if(len(NOMBRE_2) >= TAMANO_MINIMO_NOMBRES_APELLIDOS_CORPORACIONES):\n",
    "                    # adiciona los nombres encontradas al listado de nombres\n",
    "                    LISTADO_NOMBRES.append(NOMBRE_2)\n",
    "\n",
    "                # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos   \n",
    "                if(len(APELLIDO_1) >= TAMANO_MINIMO_NOMBRES_APELLIDOS_CORPORACIONES):\n",
    "                    # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos\n",
    "                    LISTADO_APELLIDOS.append(APELLIDO_1)\n",
    "\n",
    "            # verifica si solo existe NOMBRE_2 y los aAPELLIDO_1 y APELLIDO_2 a la X_ETIQUETA_COMPLETA leida\n",
    "            elif( NOMBRE_1 == '' and NOMBRE_2 != '' and APELLIDO_1 != '' and APELLIDO_2 != ''):                 \n",
    "               \n",
    "                # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos\n",
    "                if(len(NOMBRE_2) >= TAMANO_MINIMO_NOMBRES_APELLIDOS_CORPORACIONES):\n",
    "                    # adiciona los nombres encontradas al listado de nombres\n",
    "                    LISTADO_NOMBRES.append(NOMBRE_2)\n",
    "                    \n",
    "                # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos   \n",
    "                if(len(APELLIDO_1) >= TAMANO_MINIMO_NOMBRES_APELLIDOS_CORPORACIONES):\n",
    "                    # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos\n",
    "                    LISTADO_APELLIDOS.append(APELLIDO_1)\n",
    "                    \n",
    "                # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos   \n",
    "                if(len(APELLIDO_2) >= TAMANO_MINIMO_NOMBRES_APELLIDOS_CORPORACIONES):\n",
    "                    # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos\n",
    "                    LISTADO_APELLIDOS.append(APELLIDO_2)\n",
    "                    \n",
    "            # verifica si solo existe NOMBRE_1 y los dos apellidos asocioados a la X_ETIQUETA_COMPLETA leida\n",
    "            elif(NOMBRE_1 != '' and NOMBRE_2 == '' and APELLIDO_1 != '' and APELLIDO_2 != ''):\n",
    "                \n",
    "                # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos\n",
    "                if(len(NOMBRE_1) >= TAMANO_MINIMO_NOMBRES_APELLIDOS_CORPORACIONES):\n",
    "                    # adiciona los nombres encontradas al listado de nombres\n",
    "                    LISTADO_NOMBRES.append(NOMBRE_1)\n",
    "\n",
    "                # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos   \n",
    "                if(len(APELLIDO_1) >= TAMANO_MINIMO_NOMBRES_APELLIDOS_CORPORACIONES):\n",
    "                    # adiciona los apellidos encontradas al listado de apellidos\n",
    "                    LISTADO_APELLIDOS.append(APELLIDO_1)\n",
    "                    \n",
    "                 # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos   \n",
    "                if(len(APELLIDO_2) >= TAMANO_MINIMO_NOMBRES_APELLIDOS_CORPORACIONES):\n",
    "                    # adiciona los apellidos encontradas al listado de apellidos\n",
    "                    LISTADO_APELLIDOS.append(APELLIDO_2)\n",
    "                                \n",
    "            # tiene ambos nombres y apellidos en la etiqueta leida\n",
    "            else:\n",
    "\n",
    "                # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos\n",
    "                if(len(NOMBRE_1) >= TAMANO_MINIMO_NOMBRES_APELLIDOS_CORPORACIONES):\n",
    "                    # adiciona los nombres encontradas al listado de nombres\n",
    "                    LISTADO_NOMBRES.append(NOMBRE_1)\n",
    "                    \n",
    "                # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos\n",
    "                if(len(NOMBRE_2) >= TAMANO_MINIMO_NOMBRES_APELLIDOS_CORPORACIONES):\n",
    "                    # adiciona los nombres encontradas al listado de nombres\n",
    "                    LISTADO_NOMBRES.append(NOMBRE_2)\n",
    "                    \n",
    "                # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos   \n",
    "                if(len(APELLIDO_1) >= TAMANO_MINIMO_NOMBRES_APELLIDOS_CORPORACIONES):\n",
    "                    # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos\n",
    "                    LISTADO_APELLIDOS.append(APELLIDO_1)\n",
    "                    \n",
    "                 # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos   \n",
    "                if(len(APELLIDO_2) >= TAMANO_MINIMO_NOMBRES_APELLIDOS_CORPORACIONES):\n",
    "                    # valida que la etiqueta cumpla con el minimo de carateres declaradas para nombres y apellidos\n",
    "                    LISTADO_APELLIDOS.append(APELLIDO_2)\n",
    "                \n",
    "        # elimina los elementos repetidos en las listas\n",
    "        LISTADO_CORPORACIONES = list(set(LISTADO_CORPORACIONES))\n",
    "        LISTADO_NOMBRES       = list(set(LISTADO_NOMBRES))\n",
    "        LISTADO_APELLIDOS     = list(set(LISTADO_APELLIDOS))\n",
    "\n",
    "        # ordena los elementos de las listas de mayor a menor tamano en caracteres\n",
    "        LISTADO_CORPORACIONES = sorted(LISTADO_CORPORACIONES,key=len, reverse=True)\n",
    "        LISTADO_NOMBRES       = sorted(LISTADO_NOMBRES,key=len, reverse=True)\n",
    "        LISTADO_APELLIDOS     = sorted(LISTADO_APELLIDOS,key=len, reverse=True)\n",
    "\n",
    "        # lista especifica que permite guardar una cadena de cada elemento de la lista 'CORPORACIONES','NOMBRES','APELLIDOS'\n",
    "        # en forma tokenizada, para reemplazarlo posteriomente en el texto de la complementacion\n",
    "        LISTADO_CORPORACIONES_REEMPLAZABLE        = []\n",
    "        LISTADO_NOMBRES_REEMPLAZABLE              = []\n",
    "        LISTADO_APELLIDOS_REEMPLAZABLE            = []\n",
    "        LISTA_CORPORACIONES                       = []\n",
    "        LISTA_APELLIDOS                           = []\n",
    "        LISTA_NOMBRES                             = []\n",
    "        LISTADO_DE_LISTAS_ITERABLES               = [LISTA_CORPORACIONES, LISTA_APELLIDOS, LISTA_NOMBRES]\n",
    "        LISTA_DE_UNION_ORIGINALES_Y_TOKENIZADOS   = []\n",
    "\n",
    "        # variables declaradas para ser la base de los reemplazos tokenizados de cada lista usada\n",
    "        ITERADOR_CORPORACIONES_NOMBRES_APELLIDOS  = [LISTADO_CORPORACIONES,LISTADO_NOMBRES,LISTADO_APELLIDOS]\n",
    "        LISTADO_BASE_REEMPLAZO                    = ['CORPORACIONES','NOMBRES','APELLIDOS']\n",
    "        SIMBOLOS_BASE_REEMPLAZO                   = ['Ξ','Ψ','Ϡ']\n",
    "        LISTADOS_REEMPLAZABLES                    = [LISTADO_CORPORACIONES_REEMPLAZABLE,LISTADO_NOMBRES_REEMPLAZABLE,LISTADO_APELLIDOS_REEMPLAZABLE]\n",
    "\n",
    "        # recorre los listados uno a uno, corporaciones, nombres, apellidos\n",
    "        for iterador_de_todos_los_listados in range(len(ITERADOR_CORPORACIONES_NOMBRES_APELLIDOS)):\n",
    "\n",
    "            # recorre uno a uno los elementos de cada listado primero los de corporaciones, luego los de apellidos, y luego los de nombres\n",
    "            # crea unas cadenas del tamano de cada corporacion, apellido y nombres, con los simbolos correspondientes a 'CORPORACIONES','NOMBRES','APELLIDOS'\n",
    "            for iterador_de_listado_seleccionado in range(len(ITERADOR_CORPORACIONES_NOMBRES_APELLIDOS[iterador_de_todos_los_listados])):\n",
    "\n",
    "                # variable con contenido vacio, para luego crear\n",
    "                REEMPLAZO = ''\n",
    "\n",
    "                # lee el token que se va a usar como reemplazo en cada una de las listas\n",
    "                TOKEN_REEMPLAZO    = SIMBOLOS_BASE_REEMPLAZO[iterador_de_todos_los_listados]\n",
    "\n",
    "                # carga cual lista es la que se va a generar con los token de reemplazo\n",
    "                LISTA_REEMPLAZABLE = LISTADOS_REEMPLAZABLES[iterador_de_todos_los_listados]\n",
    "\n",
    "                # crea una cadena de igual tamaño para cada elemento de la lista seleccionada, con el simbolo seleccionado\n",
    "                for listado_final in range(len(ITERADOR_CORPORACIONES_NOMBRES_APELLIDOS[iterador_de_todos_los_listados][iterador_de_listado_seleccionado])):\n",
    "                    REEMPLAZO += TOKEN_REEMPLAZO\n",
    "\n",
    "                # une el contenido generado de los elementos de cada lista\n",
    "                LISTA_REEMPLAZABLE.append(REEMPLAZO)             \n",
    "        \n",
    "        # variables declaradascomo base para reemplazar los valores orginales con los tokens en la complementacion\n",
    "        LISTADO_DATOS_ORIGINALES_DE_COMPLEMENTACION     = [LISTADO_CORPORACIONES,LISTADO_APELLIDOS,LISTADO_NOMBRES]\n",
    "        LISTADO_DATOS_PARA_REEMPLAZO_EN_COMPLEMENTACION = [LISTADO_CORPORACIONES_REEMPLAZABLE,LISTADO_APELLIDOS_REEMPLAZABLE,LISTADO_NOMBRES_REEMPLAZABLE]\n",
    "        \n",
    "        # itera cada lista original y tokenizada y une los valores de sus indices en un vector\n",
    "        for lista_datos_originales_leida in range(len(LISTADO_DATOS_ORIGINALES_DE_COMPLEMENTACION)):\n",
    "            \n",
    "            listapegada = LISTADO_DE_LISTAS_ITERABLES[lista_datos_originales_leida]\n",
    "            # itera los valores contenidos dentro de cada leida corporaciones,apellidos, nombres\n",
    "            for sublista_leida in range (len(LISTADO_DATOS_ORIGINALES_DE_COMPLEMENTACION[lista_datos_originales_leida])):\n",
    "                \n",
    "                # lee los valores de las listas con datos originales y con listas de datos de remplazo\n",
    "                valor_lista_original   = LISTADO_DATOS_ORIGINALES_DE_COMPLEMENTACION[lista_datos_originales_leida][sublista_leida]\n",
    "                valor_lista_remplazo   = LISTADO_DATOS_PARA_REEMPLAZO_EN_COMPLEMENTACION[lista_datos_originales_leida][sublista_leida]\n",
    "                \n",
    "                # reemplaza los datos en la complementacion, basado en los datos leidos\n",
    "                # de la lista original y la de remplazo\n",
    "                \n",
    "                listapegada.append(valor_lista_original + ',' + valor_lista_remplazo)\n",
    "                \n",
    "        # une todas los datos generados por cada lista\n",
    "        LISTA_DE_UNION_ORIGINALES_Y_TOKENIZADOS = LISTA_CORPORACIONES + LISTA_APELLIDOS + LISTA_NOMBRES\n",
    "                \n",
    "    # ordena de mayor a menor cada la lista que unio todas las listas\n",
    "    LISTA_DE_UNION_ORIGINALES_Y_TOKENIZADOS = sorted(LISTA_DE_UNION_ORIGINALES_Y_TOKENIZADOS,key=len, reverse=True)\n",
    "\n",
    "    # itera la lista de palabras originales etiquetadas y sus versiones de reemplazo, y sustituye las frases originales por las de reemplazo\n",
    "    for posicion_lista_final_tokenizada in range (len(LISTA_DE_UNION_ORIGINALES_Y_TOKENIZADOS)):\n",
    "        \n",
    "        # lee los valores de las listas con datos originales y con listas de datos de remplazo\n",
    "        lista_reemplazo_final  = LISTA_DE_UNION_ORIGINALES_Y_TOKENIZADOS[posicion_lista_final_tokenizada].split(',')\n",
    "        \n",
    "        # obtiene el valor original y el de reemplazo para la complementacion\n",
    "        valor_lista_original   = lista_reemplazo_final[0]\n",
    "        valor_lista_remplazo   = lista_reemplazo_final[1]\n",
    "        \n",
    "        # reemplaza los datos en la complementacion, basado en los datos leidos\n",
    "        # de la lista original y la de remplazo\n",
    "        COMPLEMENTACION_TOKENIZADA = COMPLEMENTACION_TOKENIZADA.replace(valor_lista_original , valor_lista_remplazo)     \n",
    "\n",
    "    # devuelve el listado de nombres, apellidos y corporaciones leidas en la fila del archivo, \n",
    "    # ordenados por tamano, sin repetir y tambien en sus formas de reemplazo tokenizadas\n",
    "    return LISTADO_CORPORACIONES, LISTADO_CORPORACIONES_REEMPLAZABLE, LISTADO_APELLIDOS, LISTADO_APELLIDOS_REEMPLAZABLE, LISTADO_NOMBRES, LISTADO_NOMBRES_REEMPLAZABLE, COMPLEMENTACION, COMPLEMENTACION_TOKENIZADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion que recorre todas las filas del archivo y llama a la funcion que recorre las columnas de cada fila\n",
    "def recorrer_filas_para_obtener_listados (fila_inicial,fila_final,columnas_totales):\n",
    "\n",
    "    # almacena los listados obtenidos de nombres, apellidos y organizaciones para luego usarlos como diccionarios\n",
    "    LISTADOS_CORPORACIONES_NOMBRES_APELLIDOS_COMPLEMENTACIONES_Y_VALORES_TOKENIZADOS = []\n",
    "    \n",
    "    # recorre las filas del dataframe\n",
    "    for fila in range(fila_inicial,fila_final):       \n",
    "\n",
    "        SIMBOLO_PREPROCESAMIENTO   = '▓'\n",
    "        TEXTO_COMPLEMENTACION      = df_original['COMPLEMENTACION'].iloc[fila].upper().strip()\n",
    "        # preprocesamiento, cambio de caracter coma ',' por el SIMBOLO_PREPROCESAMIENTO para evitar fallos al usar el contenido de informacion en vectores\n",
    "        TEXTO_COMPLEMENTACION      = TEXTO_COMPLEMENTACION.replace(',',SIMBOLO_PREPROCESAMIENTO)  \n",
    "        TEXTO_COMPLEMENTACION      = TEXTO_COMPLEMENTACION.replace(\"\\'\",'').replace('\\\\','').replace('\"', '')    \n",
    "        NOMBRE_ARCHIVO_LEIDO       = df_original['ARCHIVO'].iloc[fila].upper().strip()\n",
    "\n",
    "        # llama a la funcion resultados_recorrer_columnas para obtener la informacion de las columnas de cada fila\n",
    "        resultados_obtencion_listados = obtener_listado_nombres_apellidos_corporaciones(fila, columnas_totales, NOMBRE_ARCHIVO_LEIDO, TEXTO_COMPLEMENTACION)\n",
    "        \n",
    "        # obtiene los resultados de los listados y los almacena en una lista\n",
    "        LISTADOS_CORPORACIONES_NOMBRES_APELLIDOS_COMPLEMENTACIONES_Y_VALORES_TOKENIZADOS.append(resultados_obtencion_listados)\n",
    "        \n",
    "    # retorna la lista de nombres, apellidos y organizaciones de cada OCR leido\n",
    "    return LISTADOS_CORPORACIONES_NOMBRES_APELLIDOS_COMPLEMENTACIONES_Y_VALORES_TOKENIZADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion que convierte la complementacion de los OCR en listas que son separadas cada que hay un espacio en blanco\n",
    "def preparar_complementacion_de_ocr_tokenizado_por_palabras_clave (LISTADO_OCR_TOKENIZADOS):\n",
    "    \n",
    "    # variable que almacena toda la complementacion\n",
    "    TEXTO_DEPURADO = []\n",
    "    # guarda los contenidos de cada OCR particionado\n",
    "    for ocr_numero in range(len(LISTADO_OCR_TOKENIZADOS)):\n",
    "        \n",
    "        # fracciona el texto cada que encuentre un espacio\n",
    "        TEXTO_DEPURADO.append(LISTADO_OCR_TOKENIZADOS[ocr_numero].split())\n",
    "        \n",
    "    # retorna el texto de la complementacion partido en forma de lista\n",
    "    return TEXTO_DEPURADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion que convierte la complementacion de los OCR en listas que son separadas cada que hay un espacio en blanco\n",
    "def preparar_complementacion_de_ocr_tokenizado_por_palabras_clave_coordenadas (LISTADO_OCR_TOKENIZADOS):\n",
    "    \n",
    "    # variable que almacena toda la complementacion\n",
    "    TEXTO_DEPURADO = []\n",
    "    # guarda los contenidos de cada OCR particionado\n",
    "    for ocr_numero in range(len(LISTADO_OCR_TOKENIZADOS)):\n",
    "        TEXTO_DEPURADO.append(LISTADO_OCR_TOKENIZADOS[ocr_numero].split())\n",
    "        \n",
    "    # retorna el texto de la complementacion partido en forma de lista\n",
    "    return TEXTO_DEPURADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtiene todas las complementaciones tokenizadas en forma de lista y tambien en forma de string\n",
    "def obtener_listas_con_palabras_clave (LISTADO_OCR_TOKENIZADOS): \n",
    "        \n",
    "    # va a almacenar todos los OCR particionados\n",
    "    OCR_ENTRENAMIENTO_LISTA_DE_LISTAS = []\n",
    "\n",
    "    # declara los simbolos por los cuales se va a reeemplazar cada tipo de contenido encontrado\n",
    "    VERIFICAR      = ['ΨΨ','ϠϠ','ΞΞ']\n",
    "    # declara las palabras clave con que se reemplazan los simbolos\n",
    "    ACTUALIZAR     = ['NOMBRE','APELLIDO','ORGANIZACION','☹']\n",
    "\n",
    "    # recorre la lista de listas de cada elemento de la lista creada para cada OCR\n",
    "    for ocr_leido in range(len(preparar_complementacion_de_ocr_tokenizado_por_palabras_clave(LISTADO_OCR_TOKENIZADOS))):\n",
    "\n",
    "        # lee la complementacion tokenizada que va a ser reemplazada por las palabras clave organizacion, nombre, apellido, carita triste '☹', en una lista\n",
    "        OCR_A_TRATAR = preparar_complementacion_de_ocr_tokenizado_por_palabras_clave(LISTADO_OCR_TOKENIZADOS)[ocr_leido]    \n",
    "\n",
    "        # define la posicion donde inicia la lectura de cada palabra comparada \n",
    "        POSICION_INICIAL   = 0\n",
    "        \n",
    "        # reemplaza los simbolos encontrados por las palabras clave definidas\n",
    "        for index, item in enumerate(OCR_A_TRATAR):\n",
    "\n",
    "            # verifica si es un nombre muestra la ubicacion donde termina la palabra ,adiciona el simbolo de reemplazo 'NOMBRE', concatena '=' con el tamaño del texto leido y le adiciona las posiciones donde esta la palabra leida\n",
    "            if (item.find(VERIFICAR[0])) >= 0 :\n",
    "                OCR_A_TRATAR[index] = str(POSICION_INICIAL + len(item) - 1) + '⇆' + ACTUALIZAR[0] + '=' + str(len(item)) + '[' + str(POSICION_INICIAL) + ':'  + str(POSICION_INICIAL + len(item) - 1) + ']'\n",
    "                #POSICION_INICIAL = len(item)\n",
    "                \n",
    "            # verifica si es un apellido muestra la ubicacion donde termina la palabra ,adiciona el simbolo de reemplazo 'APELLIDO', concatena '=' con el tamaño del texto leido y le adiciona las posiciones donde esta la palabra leida\n",
    "            elif (item.find(VERIFICAR[1])) >= 0 :\n",
    "                OCR_A_TRATAR[index] = str(POSICION_INICIAL + len(item) - 1) + '⇆' + ACTUALIZAR[1] + '=' + str(len(item)) + '[' + str(POSICION_INICIAL) + ':'  + str(POSICION_INICIAL + len(item) - 1) + ']'\n",
    "                #POSICION_INICIAL = len(item)\n",
    "                \n",
    "            # verifica si es una organizacion muestra la ubicacion donde termina la palabra ,adiciona el simbolo de reemplazo 'ORGANIZACION', concatena '=' con el tamaño del texto leido y le adiciona las posiciones donde esta la palabra leida\n",
    "            elif (item.find(VERIFICAR[2])) >= 0 :\n",
    "                OCR_A_TRATAR[index] = str(POSICION_INICIAL + len(item) - 1) + '⇆' + ACTUALIZAR[2] + '=' + str(len(item)) + '[' + str(POSICION_INICIAL) + ':'  + str(POSICION_INICIAL + len(item) - 1) + ']'\n",
    "                #POSICION_INICIAL = len(item)\n",
    "                \n",
    "            # como no es ninguna opcion, muestra la ubicacion donde termina la palabra ,adiciona el simbolo de reemplazo '☹', conconcatena '=' con el tamaño del texto leido y le adiciona las posiciones donde esta la palabra leida\n",
    "            else:        \n",
    "                OCR_A_TRATAR[index] = str(POSICION_INICIAL + len(item) - 1) + '⇆' + ACTUALIZAR[3] + '=' + str(len(item)) + '[' + str(POSICION_INICIAL) + ':'  + str(POSICION_INICIAL + len(item) - 1) + ']'\n",
    "                #POSICION_INICIAL = len(item)\n",
    "             \n",
    "            # actualiza la posicion del contenido leido y se le suma 1 que es el espacio en blanco entre la palabra actual y la proxima palabra \n",
    "            POSICION_INICIAL += len(item) + 1\n",
    "                \n",
    "        # agrupa los contenidos actualizados en una lista\n",
    "        OCR_ENTRENAMIENTO_LISTA_DE_LISTAS.append(OCR_A_TRATAR)\n",
    "\n",
    "    # retorna la lista actualizada con los valores organizacion, nombre, apellido, carita triste '☹', en una lista\n",
    "    return OCR_ENTRENAMIENTO_LISTA_DE_LISTAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion que obtiene todos las complementaciones de los OCR originales y las complementaciones tokenizadas\n",
    "def obtener_ocr_originales_y_ocr_tokenizados (LISTADOS_OCR_DE_CORPORACIONES_NOMBRES_APELLIDOS_COMPLEMENTACIONES_Y_VALORES_TOKENIZADOS, TOTAL_OCR_LEIDOS):\n",
    "    \n",
    "    # variable que almacena las complementaciones en forma de lista\n",
    "    LISTADO_OCR_ORIGINALES  = []\n",
    "    LISTADO_OCR_TOKENIZADOS = []\n",
    "    \n",
    "    # recorre lalista que tiene guardado los datos de la complementacion original y tokenizada\n",
    "    for ocr_leido in range(TOTAL_OCR_LEIDOS):\n",
    "        \n",
    "        # esta es la posicion donde se encuentran ubicados los datos en el return de la funcion recorrer_filas_para_obtener_listados\n",
    "        # y donde el resultado se encuentra contenido en la variable LISTADOS_OCR_DE_CORPORACIONES_NOMBRES_APELLIDOS_COMPLEMENTACIONES_Y_VALORES_TOKENIZADOS\n",
    "        posicion_ocr_original   = 6\n",
    "        posicion_ocr_tokenizado = 7\n",
    "        \n",
    "        # adiciona una a una las complementaciones que son leidas\n",
    "        LISTADO_OCR_ORIGINALES.append(LISTADOS_OCR_DE_CORPORACIONES_NOMBRES_APELLIDOS_COMPLEMENTACIONES_Y_VALORES_TOKENIZADOS[ocr_leido][posicion_ocr_original])\n",
    "        LISTADO_OCR_TOKENIZADOS.append(LISTADOS_OCR_DE_CORPORACIONES_NOMBRES_APELLIDOS_COMPLEMENTACIONES_Y_VALORES_TOKENIZADOS[ocr_leido][posicion_ocr_tokenizado])\n",
    "    \n",
    "    # retorna el listado de las complementaciones originales y tokenizadas\n",
    "    return LISTADO_OCR_ORIGINALES, LISTADO_OCR_TOKENIZADOS\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# permite dividir las complementaciones en fracciones de texto utilizables para entrenamiento\n",
    "def fraccionamiento_de_texto_en_complementacion_usando_tokens (LISTADO_OCR_ORIGINALES, LISTADO_OCR_TOKENIZADOS ):\n",
    "    \n",
    "    #variable declara para guardar todas las fracciones de texto TOKENIZADAS de los ocr leidos\n",
    "    complementaciones_fraccionadas_originales  = []\n",
    "    \n",
    "    # guarda todo los tokenizados con posiciones del original\n",
    "    complementaciones_fraccionadas_tokenizadas = []\n",
    "\n",
    "    # comienza a recorrer todas las complementaciones guardadas de los OCR\n",
    "    for complementacion_leida in range(len(LISTADO_OCR_ORIGINALES)):\n",
    "\n",
    "        # guarda la complementacion que se va a fraccionar\n",
    "        contenido_complementacion_original   = LISTADO_OCR_ORIGINALES[complementacion_leida] \n",
    "        contenido_complementacion_tokenizada = LISTADO_OCR_TOKENIZADOS[complementacion_leida]\n",
    "\n",
    "        # guarda la complementacion que se va a fraccionar\n",
    "        texto_actualizado         = contenido_complementacion_original\n",
    "\n",
    "        # declara el maximo valor de caracteres con que se DEBEN CREAR las frases , que son validas para las herramientas a trabajar\n",
    "        # este es un parametro estandar\n",
    "        limite_maximo_permitido    = 500\n",
    "\n",
    "        # declara el minimo valor de caracteres en que se BUSCAN LOS TOKEN Y CON ESO SE CREAN frases, que son validas para las herramientas a trabajar\n",
    "        # este es un parametro estandar\n",
    "        limite_minimo_permitido    = 490\n",
    "\n",
    "        ################################################################################################################################################################\n",
    "        ################################################################################################################################################################\n",
    "        ############## LA SIGUIENTE VARIABLE 'limite_frase_valida' ES PARA INGRESAR EL VALOR DEL TAMANO DE LAS FRASES QUE SE QUIEREN CREAR #############################\n",
    "        ################################################################################################################################################################\n",
    "        ################################################################################################################################################################\n",
    "        \n",
    "        # se define la cantidad de caracteres maximos en que se busca particionar la complementacion    \n",
    "        limite_frase_valida       = 150\n",
    "\n",
    "        # valida que la fraccion de texto a crear no sea muy pequena, menor de 150 caracteres, de ser asi se altera el limite de particiones necesarias en el texto\n",
    "        if(limite_frase_valida < limite_minimo_permitido):\n",
    "\n",
    "            # reasigna el valor minimo de particion de frases, basado en los parametros, confirurados como estandar\n",
    "            limite_frase_valida = limite_minimo_permitido\n",
    "\n",
    "            # calcula basado en la cantidad de caracteres a fraccionar, cuanto deberia ser el numero de recorrido maximo para completar la particion del texto\n",
    "            limite_particion_de_texto = math.ceil(len(contenido_complementacion_original)/limite_frase_valida) * 2\n",
    "\n",
    "        # valida que no supere la capacidad maxima permitida\n",
    "        elif(limite_frase_valida > limite_maximo_permitido):\n",
    "            # se define la cantidad de caracteres maximos en que se busca particionar la complementacion    \n",
    "            limite_frase_valida       = limite_maximo_permitido\n",
    "\n",
    "            # calcula basado en la cantidad de caracteres a fraccionar, cuanto deberia ser el numero de recorrido maximo para completar la particion del texto        \n",
    "            limite_particion_de_texto = math.ceil(len(contenido_complementacion_original)/limite_frase_valida) * 2\n",
    "\n",
    "        # asigna el limite de particiones, asegurando que esta hasta el rango maximo permitido para la particion de frases\n",
    "        else:\n",
    "            # calcula basado en la cantidad de caracteres a fraccionar, cuanto deberia ser el numero de recorrido maximo para completar la particion del texto        \n",
    "            limite_particion_de_texto = math.ceil(len(contenido_complementacion_original)/limite_frase_valida) * 2\n",
    "\n",
    "        # toma el tamano que se quiere\n",
    "        texto_complementacion_actualizado_original   = contenido_complementacion_original\n",
    "        texto_complementacion_actualizado_tokenizado = contenido_complementacion_tokenizada\n",
    "\n",
    "        # guarda el contenido de la complementacion particionada EN UNA LISTA\n",
    "        union_texto_orginal     = []\n",
    "        union_texto_tokenizado  = []\n",
    "\n",
    "        # almacena el contenido de la porcion de texto que se quiere verificar de tamano y ver si es necesario particionar con TOKEN \n",
    "        frase_original_a_evaluar   = '1'\n",
    "        frase_tokenizada_a_evaluar = '1'\n",
    "\n",
    "        #print('TAMANO COMPLEMENTACION = ', len(contenido_complementacion_original))\n",
    "        #print('FUE NECESARIO PARTIR EN = ', limite_particion_de_texto , ' PARTES')\n",
    "        #print()\n",
    "        #print(contenido_complementacion_original)    \n",
    "        #print()\n",
    "\n",
    "        # itera la complementacion para separarla en textos fraccionados, con el tamano que se desea\n",
    "        for particionado in range(limite_particion_de_texto):\n",
    "\n",
    "            # guarda la complementacion actualizada en contenido y tamano\n",
    "            texto_complementacion_actualizado_original   = texto_complementacion_actualizado_original\n",
    "            texto_complementacion_actualizado_tokenizado = texto_complementacion_actualizado_tokenizado\n",
    "\n",
    "            # criterio de parada para las iteraciones, si la frase tokenizada tiene un tamano de cero, es porque no hay que seguir iterando\n",
    "            if(len(frase_original_a_evaluar) > 0):            \n",
    "\n",
    "                # quiere decir que hay contenido que se puede TOKENIZAR o guardar directamente\n",
    "                # if validacion, si existe contenido en texto_complementacion_actualizado_original\n",
    "                if (len(texto_complementacion_actualizado_original) > 0):\n",
    "\n",
    "                    #print('frase_original_a_evaluar ', len(frase_original_a_evaluar)) \n",
    "                    # si el texto leido de la complementacion es menor o igual al limite permitido, se guarda\n",
    "                    # if validacion que el texto a evaluar este en el rango permitido\n",
    "                    if(len(texto_complementacion_actualizado_original) <= limite_frase_valida):\n",
    "\n",
    "                        #print('NO FUE NECESARIO TOKENIZADO')                     \n",
    "                        #print('LA PARTICION = ', particionado + 1 , ', TIENE UN TAMANO FRASE TOKENIZADA DE :', len(texto_complementacion_actualizado_original), 'CARACTERES')\n",
    "                        #print('EL UTTERANCE PARA ENTRENAMIENTO ES : ', texto_complementacion_actualizado_original)\n",
    "\n",
    "                        # guarda el texto que cumple con el tamano en una lista\n",
    "                        union_texto_orginal.append(texto_complementacion_actualizado_original)\n",
    "                        union_texto_tokenizado.append(texto_complementacion_actualizado_tokenizado)\n",
    "                        \n",
    "\n",
    "                        #actualiza el tamano del texto a leer, a cero para que no se use mas\n",
    "                        texto_complementacion_actualizado_original   = texto_complementacion_actualizado_original[:0]\n",
    "                        texto_complementacion_actualizado_tokenizado = texto_complementacion_actualizado_tokenizado[:0]\n",
    "\n",
    "                        # guarda el contenido del texto_complementacion_actualizado_original, en esta variable para poder verlo en el print\n",
    "                        # despues de que no se necesite validar con print, se puede eliminar esta variable\n",
    "                        frase_original_a_evaluar   = texto_complementacion_actualizado_original \n",
    "                        frase_tokenizada_a_evaluar = texto_complementacion_actualizado_tokenizado\n",
    "\n",
    "                    # si el tamano del texto que existe el la complementacion es mayor\n",
    "                    # al limite permitido por frase, este debe ser particionado con TOKEN\n",
    "                    # else del if validacion que el texto a evaluar este en el rango permitido\n",
    "                    else:\n",
    "                        # define la posicion donde inicia la porcion de texto que se va a evaluar para la particion con el TOKEN\n",
    "                        limite_inferior_de_texto_a_seleccionar = 0\n",
    "                        # define la posicion donde finaliza la porcion de texto que se va a evaluar para la particion con el TOKEN\n",
    "                        limite_superior_de_texto_a_seleccionar = limite_inferior_de_texto_a_seleccionar + limite_frase_valida\n",
    "                        # toma el texto que se va a evaluar, basado en los rangos anteriores\n",
    "                        frase_seleccionada_del_texto           = texto_complementacion_actualizado_original[limite_inferior_de_texto_a_seleccionar : limite_superior_de_texto_a_seleccionar]\n",
    "                        # invierte el texto para buscar la aparicion del TOKEN,desde el final hasta el comienzo\n",
    "                        frase_seleccionada_del_texto_invertida = frase_seleccionada_del_texto[::-1]\n",
    "                        # encuentra la posicion del token en el texto invertido\n",
    "                        posicion_token_encontrado              = frase_seleccionada_del_texto_invertida.find(' .')\n",
    "                        # selecciona la frase particionada con la posicion del TOKEN utilizado, PUNTO ESPACIO. se pone ' .' porque el texto esta invertido, pero en el texto normal es punto espacio\n",
    "                        frase_original_a_evaluar               = texto_complementacion_actualizado_original[limite_inferior_de_texto_a_seleccionar : limite_superior_de_texto_a_seleccionar - posicion_token_encontrado]\n",
    "                        frase_tokenizada_a_evaluar             = texto_complementacion_actualizado_tokenizado[limite_inferior_de_texto_a_seleccionar : limite_superior_de_texto_a_seleccionar - posicion_token_encontrado]\n",
    "\n",
    "                        # obtiene el ultimo caracter de la particion de la complementacion que se realizo\n",
    "                        ultimo_caracter_frase_tokenizada       = frase_original_a_evaluar[-1:]\n",
    "\n",
    "\n",
    "                        # verifica si el TOKEN utilizado para partir la frase sirve para crear una frase dentro del limite de tamano permitido, valida que el ultimo carater sea un espacio en blanco, y de ser asi guarda la frase\n",
    "                        # valida el primer criterio usado como TOKEN de particion\n",
    "                        if( (len(frase_original_a_evaluar) >= limite_frase_valida)) and (ultimo_caracter_frase_tokenizada != ' '):\n",
    "\n",
    "                            # define la posicion donde inicia la porcion de texto que se va a evaluar para la particion con el TOKEN\n",
    "                            limite_inferior_de_texto_a_seleccionar = 0\n",
    "                            # define la posicion donde finaliza la porcion de texto que se va a evaluar para la particion con el TOKEN\n",
    "                            limite_superior_de_texto_a_seleccionar = limite_inferior_de_texto_a_seleccionar + limite_frase_valida\n",
    "                            # toma el texto que se va a evaluar, basado en los rangos anteriores\n",
    "                            frase_seleccionada_del_texto           = texto_complementacion_actualizado_original[limite_inferior_de_texto_a_seleccionar : limite_superior_de_texto_a_seleccionar]\n",
    "                            # invierte el texto para buscar la aparicion del TOKEN,desde el final hasta el comienzo\n",
    "                            frase_seleccionada_del_texto_invertida = frase_seleccionada_del_texto[::-1]\n",
    "                            # encuentra la posicion del token en el texto invertido\n",
    "                            posicion_token_encontrado              = frase_seleccionada_del_texto_invertida.find('.')\n",
    "                            # selecciona la frase particionada con la posicion del TOKEN utilizado, PUNTO\n",
    "                            frase_original_a_evaluar               = texto_complementacion_actualizado_original[limite_inferior_de_texto_a_seleccionar : limite_superior_de_texto_a_seleccionar - posicion_token_encontrado]\n",
    "                            frase_tokenizada_a_evaluar             = texto_complementacion_actualizado_tokenizado[limite_inferior_de_texto_a_seleccionar : limite_superior_de_texto_a_seleccionar - posicion_token_encontrado]\n",
    "                            ultimo_caracter_frase_tokenizada       = frase_original_a_evaluar[-1:]\n",
    "\n",
    "                            # verifica si el TOKEN utilizado para partir la frase sirve para crear una frase dentro del limite de tamano permitido, valida que el ultimo carater sea un espacio en blanco, y de ser asi guarda la frase\n",
    "                            # valida el segundo criterio usado como TOKEN de particion\n",
    "                            if( (len(frase_original_a_evaluar) >= limite_frase_valida)) and (ultimo_caracter_frase_tokenizada != ' '):\n",
    "\n",
    "                                # define la posicion donde inicia la porcion de texto que se va a evaluar para la particion con el TOKEN\n",
    "                                limite_inferior_de_texto_a_seleccionar = 0\n",
    "                                # define la posicion donde finaliza la porcion de texto que se va a evaluar para la particion con el TOKEN\n",
    "                                limite_superior_de_texto_a_seleccionar = limite_inferior_de_texto_a_seleccionar + limite_frase_valida\n",
    "                                # toma el texto que se va a evaluar, basado en los rangos anteriores\n",
    "                                frase_seleccionada_del_texto           = texto_complementacion_actualizado_original[limite_inferior_de_texto_a_seleccionar : limite_superior_de_texto_a_seleccionar]\n",
    "                                # invierte el texto para buscar la aparicion del TOKEN,desde el final hasta el comienzo\n",
    "                                frase_seleccionada_del_texto_invertida = frase_seleccionada_del_texto[::-1]\n",
    "                                # encuentra la posicion del token en el texto invertido, ESPACIO EN BLANCO\n",
    "                                posicion_token_encontrado              = frase_seleccionada_del_texto_invertida.find(' ')\n",
    "                                # selecciona la frase particionada con la posicion del TOKEN utilizado\n",
    "                                frase_original_a_evaluar               = texto_complementacion_actualizado_original[limite_inferior_de_texto_a_seleccionar : limite_superior_de_texto_a_seleccionar - posicion_token_encontrado]\n",
    "                                frase_tokenizada_a_evaluar             = texto_complementacion_actualizado_tokenizado[limite_inferior_de_texto_a_seleccionar : limite_superior_de_texto_a_seleccionar - posicion_token_encontrado]\n",
    "                                ultimo_caracter_frase_tokenizada       = frase_original_a_evaluar[-1:]                \n",
    "\n",
    "                                # verifica si el TOKEN utilizado para partir la frase sirve para crear una frase dentro del limite de tamano permitido, valida que el ultimo carater sea un espacio en blanco, y de ser asi guarda la frase\n",
    "                                # valida el tercer criterio usado como TOKEN de particion\n",
    "                                if( (len(frase_original_a_evaluar) >= limite_frase_valida)) and (ultimo_caracter_frase_tokenizada != ' '):\n",
    "                                    print('NO SE PUDO TOKENIZAR. ESA FRASE ESTA MUY DIFICIL, AUMENTE EL VALOR DEL LIMITE DE TAMANO MINIMO DE FRASE')                        \n",
    "\n",
    "                                # la particion del texto con el TOKEN permitio llegar al limite de caracteres permitidos por frase\n",
    "                                # else de if que valida el segundo criterio usado como TOKEN de particio\n",
    "                                else:\n",
    "                                    #print('TOKENIZADO CON ESPACIO EN BLANCO')\n",
    "                                    # guarda el texto que cumple con el tamano en una lista\n",
    "                                    union_texto_orginal.append(frase_original_a_evaluar)\n",
    "                                    union_texto_tokenizado.append(frase_tokenizada_a_evaluar)\n",
    "                                    # toma la cantidad de carateres\n",
    "                                    posicion_de_inicio_nuevo_texto_actualizado = len(frase_original_a_evaluar)\n",
    "                                    # actualiza el contenido y el tamano del texto para que sea reevaluado, si ya no mas contenido, si necesita mas particiones con token, o tiene el tamano permitido y ya se puede guardar\n",
    "                                    texto_complementacion_actualizado_original   = texto_complementacion_actualizado_original[posicion_de_inicio_nuevo_texto_actualizado:]\n",
    "                                    texto_complementacion_actualizado_tokenizado = texto_complementacion_actualizado_tokenizado[posicion_de_inicio_nuevo_texto_actualizado:]\n",
    "\n",
    "                            # la particion del texto con el TOKEN permitio llegar al limite de caracteres permitidos por frase\n",
    "                            # else de if que valida el segundo criterio usado como TOKEN de particio\n",
    "                            else:\n",
    "                                #print('TOKENIZADO CON PUNTO')\n",
    "                                # guarda el texto que cumple con el tamano en una lista\n",
    "                                union_texto_orginal.append(frase_original_a_evaluar)\n",
    "                                union_texto_tokenizado.append(frase_tokenizada_a_evaluar)\n",
    "                                # toma la cantidad de carateres\n",
    "                                posicion_de_inicio_nuevo_texto_actualizado = len(frase_original_a_evaluar)\n",
    "                                # actualiza el contenido y el tamano del texto para que sea reevaluado, si ya no mas contenido, si necesita mas particiones con token, o tiene el tamano permitido y ya se puede guardar\n",
    "                                texto_complementacion_actualizado_original   = texto_complementacion_actualizado_original[posicion_de_inicio_nuevo_texto_actualizado:]\n",
    "                                texto_complementacion_actualizado_tokenizado = texto_complementacion_actualizado_tokenizado[posicion_de_inicio_nuevo_texto_actualizado:]\n",
    "\n",
    "                        # la particion del texto con el TOKEN permitio llegar al limite de caracteres permitidos por frase\n",
    "                        # else de if que valida el primer criterio usado como TOKEN de particion\n",
    "                        else:\n",
    "                            # guarda el texto que cumple con el tamano en una lista\n",
    "                            union_texto_orginal.append(frase_original_a_evaluar)\n",
    "                            union_texto_tokenizado.append(frase_tokenizada_a_evaluar)\n",
    "                            # toma la cantidad de carateres\n",
    "                            posicion_de_inicio_nuevo_texto_actualizado = len(frase_original_a_evaluar)\n",
    "                            # actualiza el contenido y el tamano del texto para que sea reevaluado, si ya no mas contenido, si necesita mas particiones con token, o tiene el tamano permitido y ya se puede guardar\n",
    "                            texto_complementacion_actualizado_original   = texto_complementacion_actualizado_original[posicion_de_inicio_nuevo_texto_actualizado:]\n",
    "                            texto_complementacion_actualizado_tokenizado = texto_complementacion_actualizado_tokenizado[posicion_de_inicio_nuevo_texto_actualizado:]\n",
    "\n",
    "                # else del if validacion, si existe contenido en texto_complementacion_actualizado_original\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            # else del if de validacion de tamano de frase_original_a_evaluar con el cual se controla la iteracion\n",
    "            else:\n",
    "                # permite terminar la ejecucion\n",
    "                break\n",
    "\n",
    "        # agrega una nueva complementacion fraccionada por tokenizado\n",
    "        complementaciones_fraccionadas_originales.append(union_texto_orginal) \n",
    "        complementaciones_fraccionadas_tokenizadas.append(union_texto_tokenizado)\n",
    "        \n",
    "    # retorna todas las fracciones de texto de las complementaciones\n",
    "    return complementaciones_fraccionadas_originales, complementaciones_fraccionadas_tokenizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtiene todas las complementaciones tokenizadas en forma de lista y tambien en forma de string\n",
    "def obtener_listas_con_palabras_clave_fraccionadas (COMPLEMENTACIONES_FRACCIONADAS_TOKENIZADAS): \n",
    "    \n",
    "    # une todos los ocr con coordenadas en una sola lista de listas\n",
    "    OCR_ENTRENAMIENTO_LISTAS_FRACCIONADAS_TOKENIZADAS = []\n",
    "\n",
    "    # recorre el listado de cada uno de los OCR que tienen sus listas fraccionadas\n",
    "    for ocr_tokenizado_leido in range(len(COMPLEMENTACIONES_FRACCIONADAS_TOKENIZADAS)):\n",
    "        \n",
    "        # carga las fracciones que van a ser calculadas del OCR que ya esta tokenizado y dividido\n",
    "        complementacion_fraccionada = COMPLEMENTACIONES_FRACCIONADAS_TOKENIZADAS[ocr_tokenizado_leido]\n",
    "        \n",
    "        # va a almacenar todos los OCR particionados\n",
    "        OCR_ENTRENAMIENTO_LISTA_DE_COMPLEMENTACIONES_FRACCIONADAS = []\n",
    "\n",
    "        # declara los simbolos por los cuales se va a reeemplazar cada tipo de contenido encontrado\n",
    "        VERIFICAR      = ['ΨΨ','ϠϠ','ΞΞ']\n",
    "        \n",
    "        # declara las palabras clave con que se reemplazan los simbolos\n",
    "        ACTUALIZAR     = ['NOMBRE','APELLIDO','ORGANIZACION','☹']\n",
    "\n",
    "        # recorre cada una de las particiones del OCR que se cargo\n",
    "        for particion_ocr_leida in range(len(complementacion_fraccionada)):\n",
    "\n",
    "            # lee la complementacion tokenizada que va a ser reemplazada por las palabras clave organizacion, nombre, apellido, carita triste '☹', en una lista\n",
    "            OCR_A_TRATAR = preparar_complementacion_de_ocr_tokenizado_por_palabras_clave_coordenadas (complementacion_fraccionada)[particion_ocr_leida]    \n",
    "\n",
    "            # define la posicion donde inicia la lectura de cada palabra comparada \n",
    "            POSICION_INICIAL   = 0\n",
    "\n",
    "            # reemplaza los simbolos encontrados por las palabras clave definidas\n",
    "            for index, item in enumerate(OCR_A_TRATAR):\n",
    "\n",
    "                # verifica si es un nombre muestra la ubicacion donde termina la palabra ,adiciona el simbolo de reemplazo 'NOMBRE', concatena '=' con el tamaño del texto leido y le adiciona las posiciones donde esta la palabra leida\n",
    "                if (item.find(VERIFICAR[0])) >= 0 :\n",
    "                    OCR_A_TRATAR[index] = str(POSICION_INICIAL + len(item) - 1) + '⇆' + ACTUALIZAR[0] + '=' + str(len(item)) + '[' + str(POSICION_INICIAL) + ':'  + str(POSICION_INICIAL + len(item) - 1) + ']'\n",
    "                    #POSICION_INICIAL = len(item)\n",
    "                    \n",
    "                # verifica si es un apellido muestra la ubicacion donde termina la palabra ,adiciona el simbolo de reemplazo 'APELLIDO', concatena '=' con el tamaño del texto leido y le adiciona las posiciones donde esta la palabra leida\n",
    "                elif (item.find(VERIFICAR[1])) >= 0 :\n",
    "                    OCR_A_TRATAR[index] = str(POSICION_INICIAL + len(item) - 1) + '⇆' + ACTUALIZAR[1] + '=' + str(len(item)) + '[' + str(POSICION_INICIAL) + ':'  + str(POSICION_INICIAL + len(item) - 1) + ']'\n",
    "                    #POSICION_INICIAL = len(item)\n",
    "\n",
    "                # verifica si es una organizacion muestra la ubicacion donde termina la palabra ,adiciona el simbolo de reemplazo 'ORGANIZACION', concatena '=' con el tamaño del texto leido y le adiciona las posiciones donde esta la palabra leida\n",
    "                elif (item.find(VERIFICAR[2])) >= 0 :\n",
    "                    OCR_A_TRATAR[index] = str(POSICION_INICIAL + len(item) - 1) + '⇆' + ACTUALIZAR[2] + '=' + str(len(item)) + '[' + str(POSICION_INICIAL) + ':'  + str(POSICION_INICIAL + len(item) - 1) + ']'\n",
    "                    #POSICION_INICIAL = len(item)\n",
    "\n",
    "                # como no es ninguna opcion, muestra la ubicacion donde termina la palabra ,adiciona el simbolo de reemplazo '☹', conconcatena '=' con el tamaño del texto leido y le adiciona las posiciones donde esta la palabra leida\n",
    "                else:\n",
    "                    OCR_A_TRATAR[index] = str(POSICION_INICIAL + len(item) - 1) + '⇆' + ACTUALIZAR[3] + '=' + str(len(item)) + '[' + str(POSICION_INICIAL) + ':'  + str(POSICION_INICIAL + len(item) - 1) + ']'\n",
    "                    #POSICION_INICIAL = len(item)\n",
    "\n",
    "                # actualiza la posicion del contenido leido y se le suma 1 que es el espacio en blanco entre la palabra actual y la proxima palabra \n",
    "                POSICION_INICIAL += len(item) + 1\n",
    "\n",
    "            # agrupa los contenidos de cada particion leida de una complementacion actualiza en una lista, con sus coordenadas\n",
    "            OCR_ENTRENAMIENTO_LISTA_DE_COMPLEMENTACIONES_FRACCIONADAS.append(OCR_A_TRATAR)\n",
    "            \n",
    "        # guarda la lista de las listas de de las complementacione fraccionadas\n",
    "        OCR_ENTRENAMIENTO_LISTAS_FRACCIONADAS_TOKENIZADAS.append(OCR_ENTRENAMIENTO_LISTA_DE_COMPLEMENTACIONES_FRACCIONADAS)\n",
    "\n",
    "    # retorna la lista actualizada con los valores organizacion, nombre, apellido, carita triste '☹', en una lista\n",
    "    return OCR_ENTRENAMIENTO_LISTAS_FRACCIONADAS_TOKENIZADAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtiene todas las complementaciones tokenizadas en forma de lista y tambien en forma de string\n",
    "def obtener_listas_con_palabras_clave_fraccionadas_tokenizadas (COMPLEMENTACIONES_FRACCIONADAS_TOKENIZADAS): \n",
    "    \n",
    "    # une todos los ocr con coordenadas en una sola lista de listas\n",
    "    OCR_ENTRENAMIENTO_LISTAS_FRACCIONADAS_TOKENIZADAS = []\n",
    "    \n",
    "    # recorre el listado de cada uno de los OCR que tienen sus listas fraccionadas\n",
    "    for ocr_tokenizado_leido in range(len(COMPLEMENTACIONES_FRACCIONADAS_TOKENIZADAS)):\n",
    "        \n",
    "        # carga las fracciones que van a ser calculadas del OCR que ya esta tokenizado y dividido\n",
    "        complementacion_fraccionada = COMPLEMENTACIONES_FRACCIONADAS_TOKENIZADAS[ocr_tokenizado_leido]\n",
    "        \n",
    "        # va a almacenar todos los OCR particionados\n",
    "        OCR_ENTRENAMIENTO_LISTA_DE_COMPLEMENTACIONES_FRACCIONADAS = []\n",
    "\n",
    "        # declara los simbolos por los cuales se va a reeemplazar cada tipo de contenido encontrado\n",
    "        VERIFICAR      = ['ΨΨ','ϠϠ','ΞΞ']\n",
    "        \n",
    "        # declara las palabras clave con que se reemplazan los simbolos\n",
    "        ACTUALIZAR     = ['NOMBRE','APELLIDO','ORGANIZACION','☹']\n",
    "        \n",
    "        # recorre cada una de las particiones del OCR que se cargo\n",
    "        for particion_ocr_leida in range(len(complementacion_fraccionada)):\n",
    "\n",
    "            # lee la complementacion tokenizada que va a ser reemplazada por las palabras clave organizacion, nombre, apellido, carita triste '☹', en una lista\n",
    "            OCR_A_TRATAR = preparar_complementacion_de_ocr_tokenizado_por_palabras_clave_coordenadas (complementacion_fraccionada)[particion_ocr_leida]  \n",
    "\n",
    "            # define la posicion donde inicia la lectura de cada palabra comparada \n",
    "            POSICION_INICIAL   = 0\n",
    "\n",
    "            # reemplaza los simbolos encontrados por las palabras clave definidas\n",
    "            for index, item in enumerate(OCR_A_TRATAR):\n",
    "\n",
    "                # verifica si es un nombre muestra la ubicacion donde termina la palabra ,adiciona el simbolo de reemplazo 'NOMBRE', concatena '=' con el tamaño del texto leido y le adiciona las posiciones donde esta la palabra leida\n",
    "                if (item.find(VERIFICAR[0])) >= 0 :\n",
    "                    OCR_A_TRATAR[index] = str(POSICION_INICIAL + len(item) - 1) + '⇆' + ACTUALIZAR[0] + '=' + str(len(item)) + '[' + str(POSICION_INICIAL) + ':'  + str(POSICION_INICIAL + len(item) - 1) + ']'\n",
    "                    #POSICION_INICIAL = len(item)\n",
    "                    \n",
    "                # verifica si es un apellido muestra la ubicacion donde termina la palabra ,adiciona el simbolo de reemplazo 'APELLIDO', concatena '=' con el tamaño del texto leido y le adiciona las posiciones donde esta la palabra leida\n",
    "                elif (item.find(VERIFICAR[1])) >= 0 :\n",
    "                    OCR_A_TRATAR[index] = str(POSICION_INICIAL + len(item) - 1) + '⇆' + ACTUALIZAR[1] + '=' + str(len(item)) + '[' + str(POSICION_INICIAL) + ':'  + str(POSICION_INICIAL + len(item) - 1) + ']'\n",
    "                    #POSICION_INICIAL = len(item)\n",
    "\n",
    "                # verifica si es una organizacion muestra la ubicacion donde termina la palabra ,adiciona el simbolo de reemplazo 'ORGANIZACION', concatena '=' con el tamaño del texto leido y le adiciona las posiciones donde esta la palabra leida\n",
    "                elif (item.find(VERIFICAR[2])) >= 0 :\n",
    "                    OCR_A_TRATAR[index] = str(POSICION_INICIAL + len(item) - 1) + '⇆' + ACTUALIZAR[2] + '=' + str(len(item)) + '[' + str(POSICION_INICIAL) + ':'  + str(POSICION_INICIAL + len(item) - 1) + ']'\n",
    "                    #POSICION_INICIAL = len(item)\n",
    "\n",
    "                # como no es ninguna opcion, muestra la ubicacion donde termina la palabra ,adiciona el simbolo de reemplazo '☹', conconcatena '=' con el tamaño del texto leido y le adiciona las posiciones donde esta la palabra leida\n",
    "                else:\n",
    "                    OCR_A_TRATAR[index] = str(POSICION_INICIAL + len(item) - 1) + '⇆' + ACTUALIZAR[3] + '=' + str(len(item)) + '[' + str(POSICION_INICIAL) + ':'  + str(POSICION_INICIAL + len(item) - 1) + ']'\n",
    "                    #POSICION_INICIAL = len(item)\n",
    "\n",
    "                # actualiza la posicion del contenido leido y se le suma 1 que es el espacio en blanco entre la palabra actual y la proxima palabra \n",
    "                POSICION_INICIAL += len(item) + 1\n",
    "\n",
    "            # agrupa los contenidos de cada particion leida de una complementacion actualiza en una lista, con sus coordenadas\n",
    "            OCR_ENTRENAMIENTO_LISTA_DE_COMPLEMENTACIONES_FRACCIONADAS.append(OCR_A_TRATAR)\n",
    "            \n",
    "        # guarda la lista de las listas de de las complementacione fraccionadas\n",
    "        OCR_ENTRENAMIENTO_LISTAS_FRACCIONADAS_TOKENIZADAS.append(OCR_ENTRENAMIENTO_LISTA_DE_COMPLEMENTACIONES_FRACCIONADAS)\n",
    "\n",
    "    # retorna la lista actualizada con los valores organizacion, nombre, apellido, carita triste '☹', en una lista\n",
    "    return OCR_ENTRENAMIENTO_LISTAS_FRACCIONADAS_TOKENIZADAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtiene el OCR con las complementaciones Tokenizadas y Originales\n",
    "def creacion_de_json_completo (COMPLEMENTACIONES_FRACCIONADAS_TOKENIZADAS, COMPLEMENTACIONES_FRACCIONADAS_ORIGINALES): \n",
    "        \n",
    "    # une todos los ocr con coordenadas en una sola lista de listas\n",
    "    OCR_ENTRENAMIENTO_LISTAS_JSON = []\n",
    "    LISTA_FINAL_JSON = ''\n",
    "    \n",
    "    # captura la informacion recibida por parametros de la funcion y las almacena en las variables declaradas\n",
    "    COMPLEMENTACIONES_FRACCIONADAS_TOKENIZADAS = COMPLEMENTACIONES_FRACCIONADAS_TOKENIZADAS\n",
    "    COMPLEMENTACIONES_FRACCIONADAS_ORIGINALES  = COMPLEMENTACIONES_FRACCIONADAS_ORIGINALES\n",
    "    \n",
    "    # declara los simbolos por los cuales se va a reeemplazar cada tipo de contenido encontrado\n",
    "    VERIFICAR      = ['ΨΨ','ϠϠ','ΞΞ']\n",
    "\n",
    "    # declara las palabras clave con que se reemplazan los simbolos\n",
    "    ACTUALIZAR     = ['NOMBRE','APELLIDO','ORGANIZACION','☹']\n",
    "    \n",
    "    # tiene la informacion del ENCABEZADO, la cadena inicial de los UTTERANCE, y la cadena con que cierra los UTTERANCE\n",
    "    NOMBRE_APLICACION               = 'TRAINING JSON'\n",
    "    CADENA_INICIAL_JSON_TRAINING    =  '{'+ '\\n' + '\"luis_schema_version\": \"3.2.0\",' + '\\n' + '\"versionId\": \"0.1\",' + '\\n' + '\"name\": \"' + NOMBRE_APLICACION + '\",\\n' + '\"desc\": \"\",' + '\\n' + '\"culture\": \"es-es\",' + '\\n' + '\"tokenizerVersion\": \"1.0.0\",' + '\\n' + '\"intents\": [' + '\\n' + '{'+ '\\n' + '\"name\": \"CTyL\"' + '\\n' + '},' + '\\n' + '{' + '\\n' + '\"name\": \"None\"' + '\\n' + '}' + '\\n' + '],' + '\\n' + '\"entities\": [' + '\\n' + '{' + '\\n' + '\"name\": \"APELLIDO\",' + '\\n' + '\"roles\": []'+ '\\n' + '},'+ '\\n' + '{'+ '\\n' + '\"name\": \"NOMBRE\",'+ '\\n' + '\"roles\": []'+ '\\n' + '},'+ '\\n' + '{'+ '\\n' + '\"name\": \"ORGANIZACION\",'+ '\\n' + '\"roles\": []'+ '\\n' + '}'+ '\\n' + '],'+ '\\n' + '\"composites\": [],'+ '\\n' + '\"closedLists\": [],'+ '\\n' + '\"patternAnyEntities\": [],'+ '\\n' + '\"regex_entities\": [],'+ '\\n' + '\"prebuiltEntities\": [],'+ '\\n' + '\"model_features\": [],'+ '\\n' + '\"regex_features\": [],'+ '\\n' + '\"patterns\": [],'+ '\\n' + '\"utterances\": ['+ '\\n'\n",
    "    CADENA_FINAL_JSON_TRAINING      =  '],'  + '\\n' + '\"settings\": []'  + '\\n' + '}' \n",
    "    CADENA_INICIAL_JSON_TESTING     =  '[' + '\\n'\n",
    "    CADENA_FINAL_JSON_TESTING       =  '\\n' +  ']' \n",
    "    CADENA_FINAL_UTTERANCE          =  '\\n'  '            ]' + '\\n' '},' + '\\n' + '\\n'\n",
    "    \n",
    "    # GUARDA TODO LOS UTTERANCE PARA DESPUES TOMAR EL 70% PARA ENTRENAMIENTO Y 30% PARA TESTING\n",
    "    LISTADO_UTTERANCE_PARA_PARTICION = []\n",
    "\n",
    "        \n",
    "    # define cual es la frase usada como intencion en la cadenqa de texto\n",
    "    INTENCION           = '\"intent\": \"CTyL\",' + '\\n'\n",
    "\n",
    "    #guarda todos las UTTERANCE en la variable\n",
    "    LISTA_INTENCIONES_JSON = ''\n",
    "    JSON_ENTRENAMIENTO     = ''\n",
    "    JSON_TESTING           = ''\n",
    "    \n",
    "    # verifica cuantas fracciones de texto no contienen etiquetas identificables\n",
    "    TOTAL_DE_FRAGMENTOS_PARA_EVALUACION_DE_ETIQUETADO = 0\n",
    "    TOTAL_DE_FRAGMENTOS_SIN_CONTENIDO_ETIQUETADO      = 0\n",
    "    BALANCE_DE_FRAGMENTOS_LEIDOS                      = []\n",
    "    \n",
    "    # lee una a una las complementaciones\n",
    "    for complementacion_leida in range(len(COMPLEMENTACIONES_FRACCIONADAS_TOKENIZADAS)):        \n",
    "        \n",
    "    \n",
    "        # carga cada OCR que ya esta TOKENIZADO y su version ORIGINAL, ambos en forma de vector\n",
    "        COMPLEMENTACION_PARTICIONADA_TOKENIZADA = COMPLEMENTACIONES_FRACCIONADAS_TOKENIZADAS[complementacion_leida]\n",
    "        COMPLEMENTACION_PARTICIONADA_ORIGINAL   = COMPLEMENTACIONES_FRACCIONADAS_ORIGINALES[complementacion_leida]\n",
    "        \n",
    "        # carga las fracciones de cada complementacion que van a ser calculadas del OCR que ya esta tokenizado y dividido\n",
    "        complementacion_fraccionada_tokenizada  = COMPLEMENTACION_PARTICIONADA_TOKENIZADA\n",
    "        complementacion_fraccionada_original    = COMPLEMENTACION_PARTICIONADA_ORIGINAL\n",
    "              \n",
    "         # lee una a una las particiones de cada complementacion\n",
    "        for particion_complementacion in range(len(complementacion_fraccionada_original)):\n",
    "            \n",
    "            # limite de UTTERANCES\n",
    "            # une todas las entidades que son leidas\n",
    "            ENTIDADES_UNION     = ''\n",
    "                        \n",
    "            # reemplaza los puntos por espacios para evitar que existan corporaciones, apellidos y nombres con el punto intermedio y los coja como una sola frase\n",
    "            # guarda la informacion del uterance con el que se va a crear el JSON\n",
    "            #UTTERANCE = complementacion_fraccionada_original[particion_complementacion].replace('.',' ').split()\n",
    "            UTTERANCE = complementacion_fraccionada_original[particion_complementacion].split()\n",
    "            UTTERANCE = ' '.join(UTTERANCE) \n",
    "\n",
    "            # reemplaza los puntos por espacios para evitar que existan corporaciones, apellidos y nombres con el punto intermedio y los coja como una sola frase\n",
    "            # construye la lista a recorrer para verficar cada palabra\n",
    "            #COMPLEMENTACION_DEPURADA_TOKENIZADA    = complementacion_fraccionada_tokenizada[particion_complementacion].replace('.',' ').split()\n",
    "            COMPLEMENTACION_DEPURADA_TOKENIZADA    = complementacion_fraccionada_tokenizada[particion_complementacion].split()\n",
    "\n",
    "            \n",
    "            # lee la complementacion tokenizada que va a ser reemplazada por las palabras clave organizacion, nombre, apellido, carita triste '☹', en una lista\n",
    "            LISTA_PARTICION_TOKENIZADA = COMPLEMENTACION_DEPURADA_TOKENIZADA\n",
    "\n",
    "            # define la posicion donde inicia la lectura de cada palabra comparada \n",
    "            POSICION_INICIAL   = 0    \n",
    "            \n",
    "            # define cual es la frase usada como intencion en la cadenqa de texto\n",
    "            INTENCION = '\"intent\": \"CTyL\",' + '\\n' + '\"intentName\": \"CTyL\",' + '\\n'\n",
    "            \n",
    "            # texto que contiene cada una de los TEXTOS que conforman el UTTERANCE leido\n",
    "            #  se adiciona los simbolos ' <>' para correcion de fallos en posiciones de algunos NOMBRES, APELLIDOS y ORGANIZACIONES, sumando tres caracteres mas al UTTERANCE, por eso el 'limite_maximo_permitido' es = 497\n",
    "            ENCABEZADO_INTENCION = '{' + '\\n' + '\"text\": ' + '\"' + UTTERANCE + ' <>' + '\"' + ', ' + '\\n' + INTENCION + '\\n' + '\"entities\"' + ': [ '  + '\\n'\n",
    "            CADENA_TEXTO_JSON    =  ''            \n",
    "\n",
    "            OTRO = ',' + '\\n\\t\\t' + '\"entityLabels\": []'\n",
    "  \n",
    "            # reemplaza los simbolos encontrados por las palabras clave definidas\n",
    "            for index, item in enumerate(LISTA_PARTICION_TOKENIZADA):\n",
    "\n",
    "                # verifica si es un nombre muestra la ubicacion donde termina la palabra ,adiciona el simbolo de reemplazo 'NOMBRE', concatena '=' con el tamaño del texto leido y le adiciona las posiciones donde esta la palabra leida\n",
    "                if (item.find(VERIFICAR[0])) >= 0 :\n",
    "\n",
    "                    CADENA_TEXTO_JSON += '\\t\\t{' + '\\n\\t\\t' + '\"entity\": \"' + ACTUALIZAR[0] + '\",' + '\\n\\t\\t' + '\"startPos\": ' + str(POSICION_INICIAL) + ',\\n\\t\\t' + '\"endPos\": '+ str(POSICION_INICIAL + len(item) - 1) + OTRO + '\\n\\t\\t' + '},' + '\\n'\n",
    "\n",
    "                # verifica si es un apellido muestra la ubicacion donde termina la palabra ,adiciona el simbolo de reemplazo 'APELLIDO', concatena '=' con el tamaño del texto leido y le adiciona las posiciones donde esta la palabra leida\n",
    "                elif (item.find(VERIFICAR[1])) >= 0 :\n",
    "\n",
    "                    CADENA_TEXTO_JSON += '\\t\\t{' + '\\n\\t\\t' + '\"entity\": \"' + ACTUALIZAR[1] + '\",' + '\\n\\t\\t' + '\"startPos\": ' + str(POSICION_INICIAL) + ',\\n\\t\\t' + '\"endPos\": '+ str(POSICION_INICIAL + len(item) - 1) + OTRO + '\\n\\t\\t' + '},' + '\\n'\n",
    "\n",
    "                # verifica si es una organizacion muestra la ubicacion donde termina la palabra ,adiciona el simbolo de reemplazo 'ORGANIZACION', concatena '=' con el tamaño del texto leido y le adiciona las posiciones donde esta la palabra leida\n",
    "                elif (item.find(VERIFICAR[2])) >= 0 :\n",
    "\n",
    "                    CADENA_TEXTO_JSON += '\\t\\t{' + '\\n\\t\\t' + '\"entity\": \"' + ACTUALIZAR[2] + '\",' + '\\n\\t\\t' + '\"startPos\": ' + str(POSICION_INICIAL) + ',\\n\\t\\t' + '\"endPos\": '+ str(POSICION_INICIAL + len(item) - 1) + OTRO + '\\n\\t\\t' + '},' + '\\n'\n",
    "\n",
    "                # como no es ninguna opcion, muestra la ubicacion donde termina la palabra ,adiciona el simbolo de reemplazo '?', conconcatena '=' con el tamaño del texto leido y le adiciona las posiciones donde esta la palabra leida\n",
    "                else:\n",
    "                    CADENA_TEXTO_JSON = CADENA_TEXTO_JSON\n",
    "\n",
    "                # actualiza la posicion del contenido leido y se le suma 1 que es el espacio en blanco entre la palabra actual y la proxima palabra \n",
    "                POSICION_INICIAL += len(item) + 1\n",
    "\n",
    "                # concatena las fracciones leidas y convertidas a JSON\n",
    "                CADENA_TEXTO_JSON += ''\n",
    "            \n",
    "            # concatena todas las complementaciones ya recorridas en particiones y estructuradas en JSON\n",
    "            #FOR RECORRE TODAS LAS COMPLEMENTACIONES\n",
    "            \n",
    "            # verifica que exista contenido en la cadena\n",
    "            if(CADENA_TEXTO_JSON != ''):\n",
    "                \n",
    "                # concatena todos los textos fraccionados en la variable y le suma su UTTERANCE\n",
    "                ENTIDADES_UNION += ENCABEZADO_INTENCION + CADENA_TEXTO_JSON[:-2] + CADENA_FINAL_UTTERANCE \n",
    "\n",
    "            # cuenta cuantas fracciones de etexto no fueron etiquetados ya que no existia ningun nombre, apellido o corporacion a etiquetar\n",
    "            else: \n",
    "                TOTAL_DE_FRAGMENTOS_SIN_CONTENIDO_ETIQUETADO += 1                \n",
    "                \n",
    "            # une todas las UTERRANCE y verifica que se identique al menos una clase NOMBRE, APELLIDO u ORGANIZACION en el texto evaluado\n",
    "            if(ENTIDADES_UNION != ''):\n",
    "                LISTA_INTENCIONES_JSON += ENTIDADES_UNION\n",
    "\n",
    "                # guarda solo las utterance\n",
    "                LISTADO_UTTERANCE_PARA_PARTICION.append(ENCABEZADO_INTENCION + CADENA_TEXTO_JSON[:-2] + CADENA_FINAL_UTTERANCE)\n",
    "                \n",
    "            #cuenta cada una de las frases fueron evaluadas para etiquetar\n",
    "            TOTAL_DE_FRAGMENTOS_PARA_EVALUACION_DE_ETIQUETADO += 1\n",
    "\n",
    "    # guarda todos los utterance\n",
    "    OCR = LISTADO_UTTERANCE_PARA_PARTICION\n",
    "    OCR_random = random.sample(OCR, len(OCR))\n",
    "\n",
    "    len_list = len(OCR)\n",
    "    len_train = math.ceil(0.7 * len_list)\n",
    "    len_test = len_list - len_train\n",
    "\n",
    "    Set_train = OCR_random[0:len_train]\n",
    "    Set_test = OCR_random[len_train:]  \n",
    "\n",
    "    # calcula cuantos datos existen repetidos en el conjunto de TRAINING y en el conjunto de TESTING\n",
    "    ELEMENTOS_REPETIDOS    = ', REPETIDOS TRAINING = ' + str(len(Set_train) - len(set(Set_train))) + ', REPETIDOS TESTING = ' + str(len(Set_test) - len(set(Set_test))) \n",
    "\n",
    "    # crea el JSON para el entrenamiento\n",
    "    JSON_ENTRENAMIENTO     = ''.join(Set_train)\n",
    "    JSON_ENTRENAMIENTO     = CADENA_INICIAL_JSON_TRAINING + JSON_ENTRENAMIENTO + CADENA_FINAL_JSON_TRAINING\n",
    "\n",
    "    # crea el json para TESTING\n",
    "    JSON_TESTING           = ''.join(Set_test)\n",
    "    JSON_TESTING           = CADENA_INICIAL_JSON_TESTING + JSON_TESTING[:-3] + CADENA_FINAL_JSON_TESTING\n",
    "    \n",
    "    \n",
    "    # guarda los valores relacionadas a las frases evaluadas y las frases utilizadas para el JSON de TRAINING y TESTING, adicionalmente la CANTIDAD DE DATOS del JSON de TRAINING y TESTING en el \n",
    "    BALANCE_DE_FRAGMENTOS_LEIDOS.append('FRAGMENTOS USADOS --> ' + str(TOTAL_DE_FRAGMENTOS_PARA_EVALUACION_DE_ETIQUETADO - TOTAL_DE_FRAGMENTOS_SIN_CONTENIDO_ETIQUETADO) + ', datos para TRAINING --> ' + str(len(Set_train)) + ', datos para TESTING --> ' + str(len(Set_test)) + ', ::::::: FRAGMENTOS LEIDOS --> ' + str(TOTAL_DE_FRAGMENTOS_PARA_EVALUACION_DE_ETIQUETADO) + ', FRAGMENTOS NO ETIQUETABLES --> ' + str(TOTAL_DE_FRAGMENTOS_SIN_CONTENIDO_ETIQUETADO) + ELEMENTOS_REPETIDOS)\n",
    "    \n",
    "    # retorna la cadena requerida con la informacion para el JSON\n",
    "    return JSON_ENTRENAMIENTO, JSON_TESTING, BALANCE_DE_FRAGMENTOS_LEIDOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion que crea los archivos '.json' para training y testing\n",
    "def creacion_ficheros_json(JSON_DATASET_TRAINING,JSON_DATASET_TESTING):\n",
    "    \n",
    "    # elimina los saltos de linea, tabulaciones y espacios del texto para crear el json    \n",
    "    fichero_training = JSON_DATASET_TRAINING.replace(\"\\n\",'').replace(\"\\t\",'').replace(\"            \",'')\n",
    "    fichero_testing  = JSON_DATASET_TESTING.replace(\"\\n\",'').replace(\"\\t\",'').replace(\"            \",'')\n",
    "\n",
    "    # nombre de los ficheros que se van a crear\n",
    "    trainingFile    = 'fichero_training.json'\n",
    "    trainingTesting = 'fichero_testing.json'\n",
    "\n",
    "    # verifica que los .json de training y testing no existan, para evitar adicionar informacion al json\n",
    "    if os.path.exists(trainingFile):\n",
    "        os.remove(trainingFile)\n",
    "        \n",
    "    if os.path.exists(trainingTesting):\n",
    "        os.remove(trainingTesting)\n",
    "    \n",
    "    \n",
    "    # crea el archivo y lo abre con compatibilidad utf-8\n",
    "    x = open('fichero_training.json','w', encoding='utf-8')\n",
    "    y = open('fichero_testing.json','w', encoding='utf-8')\n",
    "\n",
    "    # recorre la variable y la guarda en el archivo .json\n",
    "    for json_training in fichero_training:\n",
    "        x.write(json_training)\n",
    "        \n",
    "    # cierra el fichero creado\n",
    "    x.close()\n",
    "    \n",
    "    # recorre la variable y la guarda en el archivo .json\n",
    "    for json_testing in fichero_testing:\n",
    "        y.write(json_testing)\n",
    "        \n",
    "    # cierra el fichero creado\n",
    "    y.close()\n",
    "    \n",
    "    # mensaje de finalizacion en la creacion de los archivos\n",
    "    return \"Creacion Exitosa de Ficheros\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion que evalua los textos ingresados y muestra las metricas y sus identificadores\n",
    "def validacion_entidades(CLAVE_CONEXION_API, ID_API_VALIDAR, texto_validar):\n",
    "    \n",
    "    # obtiene el valor de las variables de conexion a LUIS\n",
    "    Authoring_Key  = CLAVE_CONEXION_API\n",
    "    Application_ID = ID_API_VALIDAR\n",
    "    \n",
    "    # Authoring Key\n",
    "    # son los valores que se deben poner en Ocp-Apim-Subscription-Key y en el final del request\n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Ocp-Apim-Subscription-Key': Authoring_Key,\n",
    "    }\n",
    "\n",
    "    params ={\n",
    "        # Query parameter\n",
    "        'q': texto_validar,\n",
    "        # Optional request parameters, set to default values\n",
    "        'timezoneOffset': '0',\n",
    "        'verbose': 'false',\n",
    "        'spellCheck': 'false',\n",
    "        'staging': 'false',\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r = requests.get('https://westus.api.cognitive.microsoft.com/luis/v2.0/apps/'+ Application_ID,headers=headers, params=params)\n",
    "        #print(r.json())\n",
    "        \n",
    "        return r.json()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; font-size:250%;\" font size=\"60\" >  <font color=\"red\">Las siguientes celdas, contienen toda la informacion de las variables que van a ser usadas como pararametros en las funciones </font> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guarda el nombre del archivo a leer\n",
    "archivo_lectura    = 'FORMATO ETIQUETADOR ACTUALIZADO.xlsx'\n",
    "\n",
    "# VARIABLES QUE CONTIENEN LAS CREDENCIALES DE ACCESO A LA API CREADA EN LUIS\n",
    "# Authoring Key, es la clave para conectarse a la cuenta de LUIS asociada, esta en Keys and Endpoint settings\n",
    "CLAVE_CONEXION_API = 'XXXXXXXXXXX'\n",
    "\n",
    "# Application ID, es el identificador del Modelo que se va a validar, esta en Application Information\n",
    "ID_API_VALIDAR     = 'XXXXXXXXXX2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtiene todos los datos del archivo y los guarda en la variable\n",
    "contenido_base   = obtener_datos_de_archivo(archivo_lectura)\n",
    "\n",
    "# guarda el valor de contenido_base en la variable df, para mantener las declaraciones recomendadas por la documentacion\n",
    "df_original      = contenido_base\n",
    "\n",
    "# reordena el nombre de las columnas para el procesamiento posterior\n",
    "df_original      = df_original.sort_index(axis=1)\n",
    "\n",
    "# almacena los caracteres del codigo ASCII en una lista\n",
    "caracteres_ascii = list(string.printable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# almacena los limites de filas y columnas que se leen del archivo, en tipo de dato tupla\n",
    "limites            = obtener_datos_para_control_de_contenido(df_original)\n",
    "\n",
    "# obtiene el valor total de las filas que se van a recorrer en el archivo leido\n",
    "filas_totales      = limites[0]\n",
    "\n",
    "# obtiene el valor total de las columnas que se van a recorrer en la fila leida\n",
    "columnas_totales   = limites[1]\n",
    "\n",
    "# fila dondecomienza el recorrido del archivo, se inicia desde la posicion cero para el recorrido de la primera fila\n",
    "fila_inicial       = 0\n",
    "\n",
    "# fila donde termina el recorrido del archivo\n",
    "fila_final         = filas_totales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# almacena el contenido de los listados obtenidos\n",
    "LISTADOS_OCR_DE_CORPORACIONES_NOMBRES_APELLIDOS_COMPLEMENTACIONES_Y_VALORES_TOKENIZADOS = recorrer_filas_para_obtener_listados (fila_inicial,fila_final,columnas_totales)\n",
    "\n",
    "# verifica la cantidad de OCR que fueron guardados en las listas\n",
    "TOTAL_OCR_LEIDOS                            = len(recorrer_filas_para_obtener_listados (fila_inicial,fila_final,columnas_totales))\n",
    "\n",
    "# guarda todos los ocr orignales y tokenizados en una lista\n",
    "LISTADO_DE_OCR_ORIGINALES_Y_TOKENIZADOS     = obtener_ocr_originales_y_ocr_tokenizados (LISTADOS_OCR_DE_CORPORACIONES_NOMBRES_APELLIDOS_COMPLEMENTACIONES_Y_VALORES_TOKENIZADOS, TOTAL_OCR_LEIDOS)\n",
    "\n",
    "# almacena toos los OCR que son de la version ORIGINAL del archivo leido\n",
    "LISTADO_OCR_ORIGINALES                      = LISTADO_DE_OCR_ORIGINALES_Y_TOKENIZADOS[0]\n",
    "\n",
    "# almacena todos los OCR que son la version TOKENIZADA del OCR original\n",
    "LISTADO_OCR_TOKENIZADOS                     = LISTADO_DE_OCR_ORIGINALES_Y_TOKENIZADOS[1]\n",
    "\n",
    "# almacena las complementaciones en forma de lista y texto en una lista\n",
    "LISTADO_COMPLEMENTACIONES_TOKENIZADAS       = obtener_listas_con_palabras_clave(LISTADO_OCR_TOKENIZADOS)\n",
    "\n",
    "# almacena las complementaciones tokenizadas en forma de lista\n",
    "COMPLEMENTACIONES_REEMPLAZADAS_LISTA        = LISTADO_COMPLEMENTACIONES_TOKENIZADAS\n",
    "\n",
    "# obtiene todos los valores de las complementaciones fraccionadas por los TOKEN utilizados, PUNTO ESPACIO, PUNTO, ESPACIO EN BLANCO.\n",
    "COMPLEMENTACIONES_FRACCIONADAS_ORIGINALES   = fraccionamiento_de_texto_en_complementacion_usando_tokens(LISTADO_OCR_ORIGINALES,LISTADO_OCR_TOKENIZADOS)[0]\n",
    "\n",
    "# obtiene todos los valores de los textos fraccionadas que corresponden a la complementacion original que se quiere evaluar.\n",
    "COMPLEMENTACIONES_FRACCIONADAS_TOKENIZADAS  = fraccionamiento_de_texto_en_complementacion_usando_tokens(LISTADO_OCR_ORIGINALES,LISTADO_OCR_TOKENIZADOS)[1]\n",
    "    \n",
    "# almacena las complementaciones tokenizadas en forma de cadena\n",
    "COMPLEMENTACIONES_REEMPLAZADAS_CADENA_TEXTO = []\n",
    "\n",
    "# obtiene todas las complementaciones fracccionadas con sus respecitivas coordenadas de identificacion de frases\n",
    "COMPLEMENTACIONES_COORDENADAS_TOKENIZADAS   = obtener_listas_con_palabras_clave_fraccionadas_tokenizadas (COMPLEMENTACIONES_FRACCIONADAS_TOKENIZADAS)\n",
    "\n",
    "# obtiene los datos de TRAINING Y TESTING en formato JSON, adicionalmente se toman las estadisticas de Los FRAGMENTOS TOTALES de las complementaciones leidas del achivo\n",
    "JSON_DATASET_TRAINING                       = creacion_de_json_completo (COMPLEMENTACIONES_FRACCIONADAS_TOKENIZADAS, COMPLEMENTACIONES_FRACCIONADAS_ORIGINALES)[0]\n",
    "JSON_DATASET_TESTING                        = creacion_de_json_completo (COMPLEMENTACIONES_FRACCIONADAS_TOKENIZADAS, COMPLEMENTACIONES_FRACCIONADAS_ORIGINALES)[1]\n",
    "JSON_DATASET_ESTADISTICAS_DE_DATOS_TOTALES  = creacion_de_json_completo (COMPLEMENTACIONES_FRACCIONADAS_TOKENIZADAS, COMPLEMENTACIONES_FRACCIONADAS_ORIGINALES)[2]\n",
    "\n",
    "# crea los archivos .json de training y testing en donde esta este notebook\n",
    "GENERACION_FICHEROS_JSON                    = creacion_ficheros_json(JSON_DATASET_TRAINING,JSON_DATASET_TESTING)\n",
    "\n",
    "# muestra los JSON de entrenamiento y testing para tomar y pegar en LUIS manualmente\n",
    "JSON_ENTRENAMIENTO_Y_JSON_TESTING          = creacion_de_json_completo (COMPLEMENTACIONES_FRACCIONADAS_TOKENIZADAS, COMPLEMENTACIONES_FRACCIONADAS_ORIGINALES)\n",
    "\n",
    "# itera la lista COMPLEMENTACIONES_REEMPLAZADAS_LISTA  y la convierte en cadena para ser mas comparable a la vista\n",
    "for lista_complementacion_leida in range (len(COMPLEMENTACIONES_REEMPLAZADAS_LISTA)):\n",
    "\n",
    "    # adiciona los elementos los valores de la lista\n",
    "    COMPLEMENTACIONES_REEMPLAZADAS_CADENA_TEXTO.append(' '.join(COMPLEMENTACIONES_REEMPLAZADAS_LISTA[lista_complementacion_leida]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; font-size:250%;\" font size=\"60\" >  <font color=\"red\">La siguiente celda, línea dos, la variable OCR_A_BUSCAR_NUMERO, declara un valor que permite</font> </p>\n",
    "<p style=\"text-align: justify; font-size:250%;\" font size=\"60\" >  <font color=\"red\">la comprobacion de contenidos, de las variables declaradas tambien en la próxima celda</font> </p>\n",
    "<p style=\"text-align: justify; font-size:250%;\" font size=\"60\" >  Declarar un valor, para cargar el contenido del archivo al cúal se quiere hacer seguimiento</p>\n",
    "<p style=\"text-align: justify; font-size:250%;\" font size=\"60\" >  <font color=\"red\">NOTA: </font>  los contenidos se almacenan en una lista, por esto la numeración comienza en <font color=\"red\"> CERO </font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permite traer toda la informacion relacionada de la fila del archivo que se va a cargar\n",
    "OCR_A_BUSCAR_NUMERO = 0\n",
    "\n",
    "# obtiene el nombre del OCR que se esta evaluando\n",
    "NOMBRE_DEL_OCR_LEIDO = df_original['ARCHIVO'][OCR_A_BUSCAR_NUMERO]\n",
    "\n",
    "# obtiene los listados de corporaciones, nombres y apellidos de cada OCR del archivo\n",
    "OCR_DE_CORPORACIONES_NOMBRES_APELLIDOS_COMPLEMENTACIONES_Y_VALORES_TOKENIZADOS = LISTADOS_OCR_DE_CORPORACIONES_NOMBRES_APELLIDOS_COMPLEMENTACIONES_Y_VALORES_TOKENIZADOS[OCR_A_BUSCAR_NUMERO]\n",
    "\n",
    "# obtiene el listado de corporaciones sin repetir y en orden de tamaño por cadena de cada OCR del archivo y sus tokenizados de reemplazo\n",
    "OCR_CORPORACIONES                           = OCR_DE_CORPORACIONES_NOMBRES_APELLIDOS_COMPLEMENTACIONES_Y_VALORES_TOKENIZADOS[0]\n",
    "OCR_CORPORACIONES_TOKENIZADAS               = OCR_DE_CORPORACIONES_NOMBRES_APELLIDOS_COMPLEMENTACIONES_Y_VALORES_TOKENIZADOS[1]\n",
    "\n",
    "# obtiene el listado de apellidos sin repetir y en orden de tamaño por cadena de cada OCR del archivo y sus tokenizados de reemplazo\n",
    "OCR_APELLIDOS                               = OCR_DE_CORPORACIONES_NOMBRES_APELLIDOS_COMPLEMENTACIONES_Y_VALORES_TOKENIZADOS[2]\n",
    "OCR_APELLIDOS_TOKENIZADOS                   = OCR_DE_CORPORACIONES_NOMBRES_APELLIDOS_COMPLEMENTACIONES_Y_VALORES_TOKENIZADOS[3]\n",
    "\n",
    "# obtiene el listado de nombres sin repetir y en orden de tamaño por cadena de cada OCR del archivo y sus tokenizados de reemplazo\n",
    "OCR_NOMBRES                                 = OCR_DE_CORPORACIONES_NOMBRES_APELLIDOS_COMPLEMENTACIONES_Y_VALORES_TOKENIZADOS[4]\n",
    "OCR_NOMBRES_TOKENIZADOS                     = OCR_DE_CORPORACIONES_NOMBRES_APELLIDOS_COMPLEMENTACIONES_Y_VALORES_TOKENIZADOS[5]\n",
    "\n",
    "# obtiene la complementacion orginal de cada OCR\n",
    "OCR_COMPLEMENTACION_ORIGINAL                = OCR_DE_CORPORACIONES_NOMBRES_APELLIDOS_COMPLEMENTACIONES_Y_VALORES_TOKENIZADOS[6]\n",
    "# obtiene la complementacion orginal de cada OCR ya tokenizado\n",
    "OCR_COMPLEMENTACION_TOKENIZADA              = OCR_DE_CORPORACIONES_NOMBRES_APELLIDOS_COMPLEMENTACIONES_Y_VALORES_TOKENIZADOS[7]\n",
    "\n",
    "# obtiene la complementacion orginal de cada OCR ya tokenizado, para reemplazar con lista de nombre, apellido, organizacion\n",
    "OCR_COMPLEMENTACION_TOKENIZADA_PARA_LISTA   = OCR_DE_CORPORACIONES_NOMBRES_APELLIDOS_COMPLEMENTACIONES_Y_VALORES_TOKENIZADOS[7]\n",
    "\n",
    "# obtiene la complementacion que ya fue tokenizada, con reemplazo por frase corporacion, nombre, apellido, CARA TRISTE '☹', en forma de una cadena texto\n",
    "OCR_COMPLEMENTACION_REEMPLAZADA_TEXTO       = COMPLEMENTACIONES_REEMPLAZADAS_CADENA_TEXTO[OCR_A_BUSCAR_NUMERO]\n",
    "\n",
    "# obtiene la complementacion que ya fue tokenizada, con reemplazo por frase corporacion, nombre, apellido, CARA TRISTE '☹', en forma de una lista\n",
    "OCR_COMPLEMENTACION_REEMPLAZADA_LISTA       = COMPLEMENTACIONES_REEMPLAZADAS_LISTA[OCR_A_BUSCAR_NUMERO]\n",
    "\n",
    "# obtiene todas las fracciones de texto de la complementacion original a verificar.\n",
    "OCR_COMPLEMENTACION_FRACCIONADA_ORIGINAL    = COMPLEMENTACIONES_FRACCIONADAS_ORIGINALES[OCR_A_BUSCAR_NUMERO]\n",
    "\n",
    "# obtiene todas las fracciones de texto de la complementacion original a verificar.\n",
    "OCR_COMPLEMENTACION_FRACCIONADA_TOKENIZADA  = COMPLEMENTACIONES_FRACCIONADAS_TOKENIZADAS[OCR_A_BUSCAR_NUMERO]\n",
    "\n",
    "#obtiene la complementacion fraccionada con las coordenadas de cada frase\n",
    "OCR_COMPLEMENTACION_COORDENADAS_TOKENIZADAS = COMPLEMENTACIONES_COORDENADAS_TOKENIZADAS[OCR_A_BUSCAR_NUMERO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; font-size:250%;\" font size=\"60\" >  <font color=\"red\">la traducción de símbolos que han reemplazado parte del texo son de la siguiente manera :</font> </p>\n",
    "<p style=\"text-align: justify; font-size:250%;\" font size=\"60\" >  donde están los símbolos <font color=\"red\"> Ξ </font> se refiere a que en esa posición se encontró una <font color=\"red\"> ORGANIZACIÓN </font> </p>\n",
    "<p style=\"text-align: justify; font-size:250%;\" font size=\"60\" >  donde están los símbolos <font color=\"red\"> Ψ </font> se refiere a que en esa posición se encontró un <font color=\"red\"> NOMBRE </font></p>\n",
    "<p style=\"text-align: justify; font-size:250%;\" font size=\"60\" >  donde están los símbolos <font color=\"red\"> Ϡ </font> se refiere a que en esa posición se encontró un <font color=\"red\"> APELLIDO </font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CA_129819628'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NOMBRE_DEL_OCR_LEIDO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; font-size:250%;\" font size=\"60\" >  el primér número antes de <font color=\"red\"> ⇆ </font> indica en que posición termina la palabra evaluada <font color=\"red\"></font></p>\n",
    "\n",
    "<p style=\"text-align: justify; font-size:250%;\" font size=\"60\" >  despues de <font color=\"red\"> ⇆ </font> indica que fue lo que se identificó <font color=\"red\"></font></p>\n",
    "\n",
    "<p style=\"text-align: justify; font-size:250%;\" font size=\"60\" >  la carita triste <font color=\"red\"> ☹ </font> se refiere a que en esa posición el <font color=\"red\"> CONTENIDO NO ES RELEVANTE </font></p>\n",
    "\n",
    "<p style=\"text-align: justify; font-size:250%;\" font size=\"60\" >  despues de <font color=\"red\"> = </font> indica el tamaño de la palabra que fue identificada <font color=\"red\"></font></p>\n",
    "<p style=\"text-align: justify; font-size:250%;\" font size=\"60\" >  dentro los corchetes <font color=\"red\"> [ : ] </font> [posición donde inicia la palabra <font color=\"red\"> <font color=\"red\"> : </font> </font>posición donde termina la palabra]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FRAGMENTOS USADOS --> 151, datos para TRAINING --> 106, datos para TESTING --> 45, ::::::: FRAGMENTOS LEIDOS --> 167, FRAGMENTOS NO ETIQUETABLES --> 16, REPETIDOS TRAINING = 0, REPETIDOS TESTING = 0']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MUESTRA LAS ESTADISTICAS EVALUADAS DEL SET DE DATOS\n",
    "JSON_DATASET_ESTADISTICAS_DE_DATOS_TOTALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MUESTRA EL DATASET DE TRAINING EN FORMATO JSON\n",
    "#print(JSON_DATASET_TRAINING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MUESTRA EL DATASET DE TESTING EN FORMATO JSON\n",
    "#print(JSON_DATASET_TESTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texto a validar cuales frases reconoce dentro del parrafo a evaluar.\n",
    "texto_validar = 'Y GERALDINE▓ DANIELLA Y MELANNIE ENCISO AOQUIRIERON POR DONACIONES Y HERENCIA DE EINSTEIN VDA DE ENCISO DANIELLA SEGUN SENTENCIA DEL 21 - 01 - 75 DEL COMISARIA CIVIL DEL CIRCUITO DE LONDON.; ETHIOPHIA▓ LUIS▓ ALBERT▓ MARCOS Y DIANNE EINSTEIN AOQUIRIERON POR DONACIONES Y HERENCIA DERECHOS DE EINSTEIN ORTIZ IGNACIO SEGUN SENTENCIA DEL 08 - 11 - 1973 DEL COMISARIA CIVIL DEL CIRCUITO DE LONDON.; CHARLIE.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Y GERALDINE▓ DANIELLA Y MELANNIE ENCISO AOQUIRIERON POR DONACIONES Y HERENCIA DE EINSTEIN VDA DE ENCISO DANIELLA SEGUN SENTENCIA DEL 21 - 01 - 75 DEL COMISARIA CIVIL DEL CIRCUITO DE LONDON.; ETHIOPHIA▓ LUIS▓ ALBERT▓ MARCOS Y DIANNE EINSTEIN AOQUIRIERON POR DONACIONES Y HERENCIA DERECHOS DE EINSTEIN ORTIZ IGNACIO SEGUN SENTENCIA DEL 08 - 11 - 1973 DEL COMISARIA CIVIL DEL CIRCUITO DE LONDON.; CHARLIE.',\n",
       " 'topScoringIntent': {'intent': 'CTyL', 'score': 0.9988055},\n",
       " 'entities': [{'entity': 'ENCISO',\n",
       "   'type': 'APELLIDO',\n",
       "   'startIndex': 33,\n",
       "   'endIndex': 38,\n",
       "   'score': 0.680563},\n",
       "  {'entity': 'de EINSTEIN',\n",
       "   'type': 'APELLIDO',\n",
       "   'startIndex': 78,\n",
       "   'endIndex': 88,\n",
       "   'score': 0.527863145},\n",
       "  {'entity': 'vda de ENCISO',\n",
       "   'type': 'APELLIDO',\n",
       "   'startIndex': 90,\n",
       "   'endIndex': 102,\n",
       "   'score': 0.7898774},\n",
       "  {'entity': 'ETHIOPHIA ▓',\n",
       "   'type': 'APELLIDO',\n",
       "   'startIndex': 191,\n",
       "   'endIndex': 200,\n",
       "   'score': 0.7815229},\n",
       "  {'entity': 'EINSTEIN',\n",
       "   'type': 'APELLIDO',\n",
       "   'startIndex': 232,\n",
       "   'endIndex': 239,\n",
       "   'score': 0.917849958},\n",
       "  {'entity': 'aoquirieron',\n",
       "   'type': 'APELLIDO',\n",
       "   'startIndex': 241,\n",
       "   'endIndex': 251,\n",
       "   'score': 0.5378976},\n",
       "  {'entity': 'de EINSTEIN',\n",
       "   'type': 'APELLIDO',\n",
       "   'startIndex': 288,\n",
       "   'endIndex': 298,\n",
       "   'score': 0.815769434},\n",
       "  {'entity': 'ortiz',\n",
       "   'type': 'APELLIDO',\n",
       "   'startIndex': 300,\n",
       "   'endIndex': 304,\n",
       "   'score': 0.824920833},\n",
       "  {'entity': 'DANIELLA',\n",
       "   'type': 'NOMBRE',\n",
       "   'startIndex': 13,\n",
       "   'endIndex': 20,\n",
       "   'score': 0.587554455},\n",
       "  {'entity': 'DANIELLA',\n",
       "   'type': 'NOMBRE',\n",
       "   'startIndex': 104,\n",
       "   'endIndex': 111,\n",
       "   'score': 0.918165565},\n",
       "  {'entity': 'luis ▓',\n",
       "   'type': 'NOMBRE',\n",
       "   'startIndex': 202,\n",
       "   'endIndex': 206,\n",
       "   'score': 0.886307538},\n",
       "  {'entity': 'ALBERT ▓',\n",
       "   'type': 'NOMBRE',\n",
       "   'startIndex': 208,\n",
       "   'endIndex': 214,\n",
       "   'score': 0.9666521},\n",
       "  {'entity': 'MARCOS',\n",
       "   'type': 'NOMBRE',\n",
       "   'startIndex': 216,\n",
       "   'endIndex': 221,\n",
       "   'score': 0.954779744},\n",
       "  {'entity': 'DIANNE',\n",
       "   'type': 'NOMBRE',\n",
       "   'startIndex': 225,\n",
       "   'endIndex': 230,\n",
       "   'score': 0.790849149},\n",
       "  {'entity': 'ignacio',\n",
       "   'type': 'NOMBRE',\n",
       "   'startIndex': 306,\n",
       "   'endIndex': 312,\n",
       "   'score': 0.9643784}]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# en esta funcion se obtiene los datos de la evaluacion del contenido a testear\n",
    "validacion_entidades(CLAVE_CONEXION_API, ID_API_VALIDAR, texto_validar)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
